{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.stats import skew\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "\n",
    "# sk learn import \n",
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor,AdaBoostRegressor\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import xgboost as xgb\n",
    "pd.set_option('display.max_columns', None) # display all columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "188318 rows for training set\n",
      "125546 rows for test set\n"
     ]
    }
   ],
   "source": [
    "#Import Data\n",
    "df_train = pd.read_csv(r\"C:\\Users\\piush\\Desktop\\Dataset\\Allstate\\train.csv\")\n",
    "df_test = pd.read_csv(r\"C:\\Users\\piush\\Desktop\\Dataset\\Allstate\\test.csv\")\n",
    "print ('data loaded')\n",
    "print (str(len(df_train))+\" rows for training set\")\n",
    "print (str(len(df_test))+\" rows for test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Median Absolute Deviation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_outlier(points, thresh = 3.5):\n",
    "    if len(points.shape) == 1:\n",
    "        points = points[:,None]\n",
    "    median = np.median(points, axis=0)\n",
    "    diff = np.sum((points - median)**2, axis=-1)\n",
    "    diff = np.sqrt(diff)\n",
    "    med_abs_deviation = np.median(diff)\n",
    "\n",
    "    modified_z_score = 0.6745 * diff / med_abs_deviation\n",
    "\n",
    "    return modified_z_score > thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Skew from SalesPrice data as required by the competition\n",
    "Select the last column as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = df_train[df_train.columns.values[-1]]\n",
    "target_log = (target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Train and Test to evaluate ranges and missing values excluding the last column\n",
    "This was done primarily to ensure that Categorical data in the training and testing data sets were consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train[df_train.columns.values[:-1]]\n",
    "df = df_train.append(df_test, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cats = []\n",
    "for col in df.columns.values:\n",
    "    if df[col].dtype == 'object':\n",
    "        cats.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create separte datasets for Continuous vs Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_cont = df.drop(cats, axis=1)\n",
    "df_cat = df[cats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Data for continuous data\n",
    "1. If any column contains more than 50 entries of missing data, drop the column\n",
    "2. If any column contains fewer that 50 entries of missing data, replace those missing values with the median for that column (the median imputation used on missing values is very crude. For example, Area features with missing values may be this way because the property does not have that feature (e.g. a pool) so it would make more sense to set this to zero. )\n",
    "3. Remove outliers using Median Absolute Deviation\n",
    "4. Calculate skewness for each variable and if greater than 0.75 transform it\n",
    "5. Apply the sklearn.Normalizer to each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in df_cont.columns.values:\n",
    "    if np.sum(df_cont[col].isnull()) > 50:\n",
    "        #print(\"Removing Column: {}\".format(col))\n",
    "        df_cont = df_cont.drop(col, axis = 1)\n",
    "    elif np.sum(df_cont[col].isnull()) > 0:\n",
    "        #print(\"Replacing with Median: {}\".format(col))\n",
    "        median = df_cont[col].median()\n",
    "        idx = np.where(df_cont[col].isnull())[0]\n",
    "        df_cont[col].iloc[idx] = median\n",
    "        \n",
    "        \n",
    "        outliers = np.where(is_outlier(df_cont[col]))\n",
    "        df_cont[col].iloc[outliers] = median\n",
    "        \n",
    "               \n",
    "        if skew(df_cont[col]) > 0.75:\n",
    "            #print(\"Skewness Detected: {}\".format(col))\n",
    "            df_cont[col] = np.log(df_cont[col])\n",
    "            df_cont[col] = df_cont[col].apply(lambda x: 0 if x == -np.inf else x)\n",
    "        \n",
    "        df_cont[col] = Normalizer().fit_transform(df_cont[col].reshape(1,-1))[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Data for Categorical Data\n",
    "1. If any column contains more than 50 entries of missing data, drop the column\n",
    "2. If any column contains fewer that 50 entries of missing data, replace those values with the 'MIA'\n",
    "3. Apply the sklearn.LabelEncoder\n",
    "4. For each categorical variable determine the number of unique values and for each, create a new column that is binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in df_cat.columns.values:\n",
    "    if np.sum(df_cat[col].isnull()) > 50:\n",
    "        df_cat = df_cat.drop(col, axis = 1)\n",
    "        continue\n",
    "    elif np.sum(df_cat[col].isnull()) > 0:\n",
    "        df_cat[col] = df_cat[col].fillna('MIA')\n",
    "        \n",
    "    df_cat[col] = LabelEncoder().fit_transform(df_cat[col])\n",
    "    \n",
    "    num_cols = df_cat[col].max()\n",
    "    for i in range(num_cols):\n",
    "        col_name = col + '_' + str(i)\n",
    "        df_cat[col_name] = df_cat[col].apply(lambda x: 1 if x == i else 0)\n",
    "        \n",
    "    df_cat = df_cat.drop(col, axis = 1)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Numeric and Categorical Datasets and Create Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new = df_cont.join(df_cat)\n",
    "\n",
    "df_train = df_new.iloc[:len(df_train) - 1]\n",
    "df_train = df_train.join(target_log)\n",
    "\n",
    "df_test = df_new.iloc[len(df_train) + 1:]\n",
    "\n",
    "X_train = df_train[df_train.columns.values[1:-1]]\n",
    "y_train = df_train[df_train.columns.values[-1]]\n",
    "\n",
    "X_test = df_test[df_test.columns.values[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the length for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1459 rows for training set\n",
      "1459 rows for test set\n"
     ]
    }
   ],
   "source": [
    "print (str(len(y_train))+\" rows for training set\")\n",
    "print (str(len(X_train))+\" rows for test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.9382\n"
     ]
    }
   ],
   "source": [
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print ('R-squared: %.4f' % model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "processors=1\n",
    "num_folds=3\n",
    "num_instances=len(X_train)\n",
    "# Define error measure for official scoring : RMSE\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better = False)\n",
    "\n",
    "kfold = KFold(n=num_instances, n_folds=num_folds, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms spot-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: (-0.020) +/- (0.004)\n",
      "Ridge: (-0.022) +/- (0.003)\n",
      "LassoCV: (-0.044) +/- (0.008)\n",
      "ElasticNetCV: (-0.044) +/- (0.008)\n",
      "LassoLars: (-0.018) +/- (0.003)\n",
      "SGD: (-103618737296652107791776291051536384.000) +/- (141140303960248657950612947344556032.000)\n",
      "Bayesian Ridge Regression: (-0.022) +/- (0.003)\n",
      "RANSAC: (-0.026) +/- (0.003)\n",
      "DTR: (-0.045) +/- (0.002)\n",
      "ABR: (-0.026) +/- (0.002)\n",
      "K-nn: (-0.067) +/- (0.004)\n",
      "ETR: (-0.024) +/- (0.003)\n",
      "RFR: (-0.024) +/- (0.002)\n",
      "GBR: (-0.018) +/- (0.003)\n",
      "XGBR: (-0.018) +/- (0.003)\n"
     ]
    }
   ],
   "source": [
    "# Prepare some basic models\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "models = []\n",
    "#Linear Regression without regularization\n",
    "models.append(('LR', linear_model.LinearRegression()))\n",
    "#Linear Regression with Ridge regularization (L2 penalty)\n",
    "models.append(('Ridge', linear_model.RidgeCV()))\n",
    "#Linear Regression with Lasso regularization \n",
    "models.append(('LassoCV', linear_model.LassoCV()))\n",
    "#Linear Regression with ElasticNet regularization (L1 and L2 penalty)\n",
    "models.append(('ElasticNetCV', linear_model.ElasticNetCV()))\n",
    "#Lasso Lars\n",
    "models.append(('LassoLars', linear_model.LassoLarsCV()))\n",
    "#Stochastic Gradient Descent\n",
    "models.append(('SGD', linear_model.SGDRegressor()))\n",
    "#Bayesian Ridge Regression\n",
    "models.append(('Bayesian Ridge Regression', linear_model.BayesianRidge()))\n",
    "# Robustly fit linear model with RANSAC algorithm\n",
    "models.append(('RANSAC', linear_model.RANSACRegressor(linear_model.LinearRegression())))\n",
    "\n",
    "#DecisionTreeRegressor\n",
    "models.append(('DTR', DecisionTreeRegressor(max_depth=4)))\n",
    "\n",
    "#AdaBoostRegressor\n",
    "models.append(('ABR', AdaBoostRegressor(DecisionTreeRegressor(max_depth=4))))\n",
    "\n",
    "#Kneighbors Regressor\n",
    "models.append(('K-nn', KNeighborsRegressor()))\n",
    "\n",
    "#Extra Forest Regressor\n",
    "models.append(('ETR', ExtraTreesRegressor(n_estimators=10)))\n",
    "#Random Forest Regressor\n",
    "models.append(('RFR', RandomForestRegressor()))\n",
    "\n",
    "#Gradient Boosting Regressor\n",
    "models.append(('GBR', GradientBoostingRegressor()))\n",
    "\n",
    "#XGB Regressor\n",
    "models.append(('XGBR', xgb.XGBRegressor()))\n",
    "\n",
    "\n",
    "# Evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scorer, n_jobs=processors)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(\"{0}: ({1:.3f}) +/- ({2:.3f})\".format(name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random forest Regressor is providing the best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Grid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0197614612153116\n",
      "{'alpha': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "lr_grid = GridSearchCV(\n",
    "    estimator = linear_model.Ridge(),\n",
    "    param_grid = dict(alpha=alphas), \n",
    "    cv = kfold, \n",
    "    scoring = scorer, \n",
    "    n_jobs = processors)\n",
    "\n",
    "lr_grid.fit(X_train, y_train)\n",
    "\n",
    "print(lr_grid.best_score_)\n",
    "print(lr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Random Search Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random search is an approach to parameter tuning that will sample algorithm parameters from a random distribution (i.e. uniform) for a fixed number of iterations. A model is constructed and evaluated for each combination of parameters chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.021673261855508927\n",
      "{'alpha': 0.068889665481824847}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from scipy.stats import uniform as sp_rand\n",
    "param_grid = {'alpha': sp_rand()}\n",
    "lr_grid = RandomizedSearchCV(\n",
    "    estimator = linear_model.Ridge(),\n",
    "    param_distributions=param_grid, \n",
    "    cv = kfold, \n",
    "    scoring = scorer, \n",
    "    n_jobs = processors)\n",
    "\n",
    "lr_grid.fit(X_train, y_train)\n",
    "\n",
    "print(lr_grid.best_score_)\n",
    "print(lr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.021859409143386545\n",
      "{'criterion': 'mse', 'max_depth': 10, 'max_features': 18, 'n_estimators': 200, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator = RandomForestRegressor(warm_start=True, random_state=seed),\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'criterion': ['mse'],\n",
    "        'max_features': [18, 20],\n",
    "        'max_depth': [8, 10],\n",
    "        'bootstrap': [True]\n",
    "    }, \n",
    "    cv = kfold, \n",
    "    scoring = scorer, \n",
    "    n_jobs = processors)\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(rf_grid.best_score_)\n",
    "print(rf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### XGBoost while calculating best metaparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.4,\n",
       "       gamma=0.03, learning_rate=0.07, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1.5, missing=None, n_estimators=1000, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0.75, reg_lambda=0.45,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=0.95)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python\n",
    "\n",
    "parameters_for_testing = {\n",
    "    'colsample_bytree':[0.4],\n",
    "    'gamma':[0.03],\n",
    "    'min_child_weight':[1.5],\n",
    "    'learning_rate':[0.07],\n",
    "    'max_depth':[3],\n",
    "    'n_estimators':[10000],\n",
    "    'reg_alpha':[0.75],\n",
    "    'reg_lambda':[0.45],\n",
    "    'subsample':[0.95]\n",
    "   \n",
    "}\n",
    "\n",
    "train_x= X_train\n",
    "train_y= y_train\n",
    "#train_dataset.drop(\"SalePrice\", axis=1, inplace=True)\n",
    "                    \n",
    "#xgb_model = xgboost.XGBRegressor()\n",
    "#gsearch1 = GridSearchCV(estimator = xgb_model, param_grid = parameters_for_testing, n_jobs=4,iid=False, cv=5)\n",
    "#gsearch1.fit(train_x,train_y)\n",
    "#gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
    "\n",
    "final_parameters = {\n",
    "    'colsample_bytree':[0.4],\n",
    "    'gamma':[0.03],\n",
    "    'min_child_weight':[1.5],\n",
    "    'learning_rate':[0.07],\n",
    "    'max_depth':[3],\n",
    "    'n_estimators':[1000],\n",
    "    'reg_alpha':[0.75],\n",
    "    'reg_lambda':[0.45],\n",
    "    'subsample':[0.95]\n",
    "   \n",
    "}\n",
    "\n",
    "best_xgb_model = xgb.XGBRegressor(colsample_bytree=0.4,\n",
    "                 gamma=0.030,                 \n",
    "                 learning_rate=0.07,\n",
    "                 max_depth=5,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=1000,                                                                    \n",
    "                 reg_alpha=0.75,\n",
    "                 reg_lambda=0.45,\n",
    "                 subsample=0.95)\n",
    "best_xgb_model.fit(train_x,train_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoLarsCV(copy_X=True, cv=20, eps=2.2204460492503131e-16,\n",
       "      fit_intercept=True, max_iter=500, max_n_alphas=1000, n_jobs=1,\n",
       "      normalize=True, positive=False, precompute='auto', verbose=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lassoLars = linear_model.LassoLarsCV(cv=20)\n",
    "model_lassoLars.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.07, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1, missing=None, n_estimators=1000, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb = xgb.XGBRegressor(n_estimators=1000, max_depth=5, learning_rate=0.07) #the params were tuned using xgb.cv\n",
    "model_xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=9, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=60, min_samples_split=1200,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=60, presort='auto',\n",
       "             random_state=10, subsample=0.8, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gbr = GradientBoostingRegressor(learning_rate=0.1, n_estimators=60,max_depth=9, min_samples_split=1200, min_samples_leaf=60, subsample=0.8, random_state=10)\n",
    "model_gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_preds = np.expm1(model_xgb.predict(X_test))\n",
    "lassoLars_preds = np.expm1(model_ridge.predict(X_test))\n",
    "gbr_preds = np.expm1(model_gbr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#http://mlwave.com/kaggle-ensembling-guide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights in the average (0.7, 0.3) are hyperparameters - I think I used a validation set to see what the best cutoff is. Basically this means I am weighting the preds from the lasso somewhat more heavily than the xgboost preds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = 0.9*lassoLars_preds + 0.3* xgb_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solution = pd.DataFrame({\"id\":df_test.Id, \"SalePrice\":preds}, columns=['id', 'SalePrice'])\n",
    "solution.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
