{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Womens Health Risk Assessment is a multi-class classification competition for finding an optimized machine learning solution that allow a young woman (age 15-30 years old) to be accurately categorized for their particular health risk. Based on the category a patient falls within, healthcare providers can offer appropriate education and training programs to help reduce the patient's reproductive health risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientID</th>\n",
       "      <th>geo</th>\n",
       "      <th>christian</th>\n",
       "      <th>muslim</th>\n",
       "      <th>hindu</th>\n",
       "      <th>other</th>\n",
       "      <th>cellphone</th>\n",
       "      <th>motorcycle</th>\n",
       "      <th>radio</th>\n",
       "      <th>cooker</th>\n",
       "      <th>...</th>\n",
       "      <th>ModCon</th>\n",
       "      <th>usecondom</th>\n",
       "      <th>hivknow</th>\n",
       "      <th>lowlit</th>\n",
       "      <th>highlit</th>\n",
       "      <th>urban</th>\n",
       "      <th>rural</th>\n",
       "      <th>single</th>\n",
       "      <th>segment</th>\n",
       "      <th>subgroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4835</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6719</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5450</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1207</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7290</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   patientID  geo  christian  muslim  hindu  other  cellphone  motorcycle  \\\n",
       "0       4835    9        0.0     0.0    1.0    0.0          0           0   \n",
       "1       6719    4        1.0     0.0    0.0    0.0          1           0   \n",
       "2       5450    2        0.0     1.0    0.0    0.0          0           0   \n",
       "3       1207    2        1.0     0.0    0.0    0.0          0           0   \n",
       "4       7290    9        0.0     0.0    1.0    0.0          1           0   \n",
       "\n",
       "   radio  cooker    ...     ModCon  usecondom  hivknow  lowlit  highlit  \\\n",
       "0      0       0    ...        NaN        NaN      NaN     NaN      NaN   \n",
       "1      1       0    ...        1.0        0.0      1.0     1.0      0.0   \n",
       "2      0       1    ...        0.0        0.0      1.0     0.0      1.0   \n",
       "3      0       0    ...        0.0        0.0      1.0     1.0      0.0   \n",
       "4      0       0    ...        0.0        NaN      NaN     0.0      0.0   \n",
       "\n",
       "   urban  rural  single  segment  subgroup  \n",
       "0      0      1     NaN        3         1  \n",
       "1      0      1     0.0        1         1  \n",
       "2      0      1     0.0        3         1  \n",
       "3      0      1     0.0        2         1  \n",
       "4      1      0     0.0        3         1  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the dataset\n",
    "df = pd.read_csv(\"WomenHealth_Training.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------------------\n",
      "TRAIN SET INFORMATION\n",
      "---------------------\n",
      "Shape of training set: (5283, 50) \n",
      "\n",
      "Column Headers: ['patientID', 'geo', 'christian', 'muslim', 'hindu', 'other', 'cellphone', 'motorcycle', 'radio', 'cooker', 'fridge', 'furniture', 'computer', 'cart', 'irrigation', 'thrasher', 'car', 'generator', 'INTNR', 'REGION_PROVINCE', 'DISTRICT', 'electricity', 'age', 'tribe', 'foodinsecurity', 'EVER_HAD_SEX', 'EVER_BEEN_PREGNANT', 'CHILDREN', 'india', 'married', 'multpart', 'educ', 'inschool', 'ownincome', 'literacy', 'religion', 'urbanicity', 'LaborDeliv', 'babydoc', 'Debut', 'ModCon', 'usecondom', 'hivknow', 'lowlit', 'highlit', 'urban', 'rural', 'single', 'segment', 'subgroup'] \n",
      "\n",
      "patientID               int64\n",
      "geo                     int64\n",
      "christian             float64\n",
      "muslim                float64\n",
      "hindu                 float64\n",
      "other                 float64\n",
      "cellphone               int64\n",
      "motorcycle              int64\n",
      "radio                   int64\n",
      "cooker                  int64\n",
      "fridge                  int64\n",
      "furniture               int64\n",
      "computer                int64\n",
      "cart                    int64\n",
      "irrigation              int64\n",
      "thrasher                int64\n",
      "car                     int64\n",
      "generator               int64\n",
      "INTNR                   int64\n",
      "REGION_PROVINCE         int64\n",
      "DISTRICT                int64\n",
      "electricity             int64\n",
      "age                     int64\n",
      "tribe                   int64\n",
      "foodinsecurity          int64\n",
      "EVER_HAD_SEX            int64\n",
      "EVER_BEEN_PREGNANT      int64\n",
      "CHILDREN                int64\n",
      "india                   int64\n",
      "married               float64\n",
      "multpart              float64\n",
      "educ                  float64\n",
      "inschool              float64\n",
      "ownincome             float64\n",
      "literacy              float64\n",
      "religion               object\n",
      "urbanicity              int64\n",
      "LaborDeliv            float64\n",
      "babydoc               float64\n",
      "Debut                 float64\n",
      "ModCon                float64\n",
      "usecondom             float64\n",
      "hivknow               float64\n",
      "lowlit                float64\n",
      "highlit               float64\n",
      "urban                   int64\n",
      "rural                   int64\n",
      "single                float64\n",
      "segment                 int64\n",
      "subgroup                int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (\"\\n\\n---------------------\")\n",
    "print (\"TRAIN SET INFORMATION\")\n",
    "print (\"---------------------\")\n",
    "print (\"Shape of training set:\", df.shape, \"\\n\")\n",
    "print (\"Column Headers:\", list(df.columns.values), \"\\n\")\n",
    "print (df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET INFORMATION\n",
      "========================\n",
      "\n",
      "'patientID' has 5283 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[4835 6719 5450 1207 7290  620 7760 5437  983 7491]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'geo' has 9 unique values\n",
      "[9 4 2 3 8 6 1 5 7]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'christian' has 3 unique values\n",
      "[  0.   1.  nan]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'muslim' has 3 unique values\n",
      "[  0.   1.  nan]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'hindu' has 3 unique values\n",
      "[  1.   0.  nan]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'other' has 3 unique values\n",
      "[  0.   1.  nan]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'cellphone' has 2 unique values\n",
      "[0 1]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'motorcycle' has 2 unique values\n",
      "[0 1]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'radio' has 2 unique values\n",
      "[0 1]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'cooker' has 2 unique values\n",
      "[0 1]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'fridge' has 2 unique values\n",
      "[0 1]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'furniture' has 2 unique values\n",
      "[0 1]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'computer' has 2 unique values\n",
      "[0 1]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'cart' has 2 unique values\n",
      "[0 1]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'irrigation' has 2 unique values\n",
      "[0 1]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'thrasher' has 2 unique values\n",
      "[0 1]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'car' has 2 unique values\n",
      "[0 1]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'generator' has 2 unique values\n",
      "[0 1]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'INTNR' has 3907 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[11180660     2958      737     1053  4270306     1450      144      313\n",
      " 14460825     1275]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'REGION_PROVINCE' has 67 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[48 14 43 37 23 44 38 31 50 33]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'DISTRICT' has 181 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[163 126 158 152 194 151 153 142 165 147]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'electricity' has 3 unique values\n",
      "[1 2 3]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'age' has 16 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[30 25 19 18 28 21 29 16 26 22]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'tribe' has 59 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[23 38 52 25 50 12 13 63 22  6]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'foodinsecurity' has 4 unique values\n",
      "[ 2  3  1 99]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'EVER_HAD_SEX' has 2 unique values\n",
      "[1 0]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'EVER_BEEN_PREGNANT' has 2 unique values\n",
      "[1 0]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'CHILDREN' has 2 unique values\n",
      "[1 0]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'india' has 2 unique values\n",
      "[1 0]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'married' has 3 unique values\n",
      "[  0.   1.  nan]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'multpart' has 4 unique values\n",
      "[ nan   1.   0.   2.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'educ' has 7 unique values\n",
      "[  0.   1.   2.   3.   4.   5.  nan]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'inschool' has 3 unique values\n",
      "[  0.   1.  nan]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'ownincome' has 3 unique values\n",
      "[  1.   0.  nan]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'literacy' has 7 unique values\n",
      "[ nan   0.   4.   3.   2.   5.   1.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'religion' has 11 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['Hindu' 'Evangelical/Bo' 'Muslim' 'Roman Catholic' 'Other Christia'\n",
      " 'Buddhist' 'Russian/Easter' 'Traditional/An' 'Other' nan]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'urbanicity' has 3 unique values\n",
      "[0 2 1]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'LaborDeliv' has 3 unique values\n",
      "[  1.   0.  nan]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'babydoc' has 11 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[ 0.  2.  4.  3.  5.  9.  1.  8.  6.  7.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'Debut' has 22 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[ 17.  14.  18.  nan  15.  16.  21.  20.  25.  29.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'ModCon' has 3 unique values\n",
      "[ nan   1.   0.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'usecondom' has 3 unique values\n",
      "[ nan   0.   1.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'hivknow' has 3 unique values\n",
      "[ nan   1.   0.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'lowlit' has 3 unique values\n",
      "[ nan   1.   0.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'highlit' has 3 unique values\n",
      "[ nan   0.   1.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'urban' has 2 unique values\n",
      "[0 1]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'rural' has 2 unique values\n",
      "[1 0]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'single' has 2 unique values\n",
      "[ nan   0.]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'segment' has 4 unique values\n",
      "[3 1 2 4]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'subgroup' has 2 unique values\n",
      "[1 2]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Features with missing values:\n",
      "['christian has 15 missing', 'muslim has 15 missing', 'hindu has 15 missing', 'other has 15 missing', 'married has 22 missing', 'multpart has 1157 missing', 'educ has 90 missing', 'inschool has 17 missing', 'ownincome has 60 missing', 'literacy has 116 missing', 'religion has 15 missing', 'LaborDeliv has 2520 missing', 'babydoc has 67 missing', 'Debut has 572 missing', 'ModCon has 319 missing', 'usecondom has 2367 missing', 'hivknow has 608 missing', 'lowlit has 116 missing', 'highlit has 116 missing', 'single has 1386 missing']\n",
      "\n",
      "\n",
      "Features with non-numeric values:\n",
      "['religion']\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "missing_values = []\n",
    "nonumeric_values = []\n",
    "\n",
    "print (\"TRAINING SET INFORMATION\")\n",
    "print (\"========================\\n\")\n",
    "\n",
    "for column in df:\n",
    "    # Find all the unique feature values\n",
    "    uniq = df[column].unique()\n",
    "    print (\"'{}' has {} unique values\" .format(column,uniq.size))\n",
    "    if (uniq.size > 10):\n",
    "        print(\"~~Listing up to 10 unique values~~\")\n",
    "    print (uniq[0:10])\n",
    "    print (\"\\n-----------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    # Find features with missing values\n",
    "    if (True in pd.isnull(uniq)):\n",
    "        s = \"{} has {} missing\" .format(column, pd.isnull(df[column]).sum())\n",
    "        missing_values.append(s)\n",
    "    \n",
    "    # Find features with non-numeric values\n",
    "    for i in range (1, np.prod(uniq.shape)):\n",
    "        if (re.match('nan', str(uniq[i]))):\n",
    "            break\n",
    "        if not (re.search('(^\\d+\\.?\\d*$)|(^\\d*\\.?\\d+$)', str(uniq[i]))):\n",
    "            nonumeric_values.append(column)\n",
    "            break\n",
    "  \n",
    "print (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "print (\"Features with missing values:\\n{}\\n\\n\" .format(missing_values))\n",
    "print (\"Features with non-numeric values:\\n{}\" .format(nonumeric_values))\n",
    "print (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the rows with nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientID</th>\n",
       "      <th>geo</th>\n",
       "      <th>christian</th>\n",
       "      <th>muslim</th>\n",
       "      <th>hindu</th>\n",
       "      <th>other</th>\n",
       "      <th>cellphone</th>\n",
       "      <th>motorcycle</th>\n",
       "      <th>radio</th>\n",
       "      <th>cooker</th>\n",
       "      <th>...</th>\n",
       "      <th>ModCon</th>\n",
       "      <th>usecondom</th>\n",
       "      <th>hivknow</th>\n",
       "      <th>lowlit</th>\n",
       "      <th>highlit</th>\n",
       "      <th>urban</th>\n",
       "      <th>rural</th>\n",
       "      <th>single</th>\n",
       "      <th>segment</th>\n",
       "      <th>subgroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>7060</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>1319</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>7159</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>679</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>4085</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>3582</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3321</th>\n",
       "      <td>2253</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>5106</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036</th>\n",
       "      <td>5896</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4421</th>\n",
       "      <td>8043</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4461</th>\n",
       "      <td>1965</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4722</th>\n",
       "      <td>344</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4860</th>\n",
       "      <td>7198</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4960</th>\n",
       "      <td>4485</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5125</th>\n",
       "      <td>8007</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      patientID  geo  christian  muslim  hindu  other  cellphone  motorcycle  \\\n",
       "323        7060    7        NaN     NaN    NaN    NaN          0           0   \n",
       "453        1319    8        NaN     NaN    NaN    NaN          1           0   \n",
       "685        7159    7        NaN     NaN    NaN    NaN          1           0   \n",
       "1191        679    2        NaN     NaN    NaN    NaN          0           0   \n",
       "1887       4085    2        NaN     NaN    NaN    NaN          1           1   \n",
       "1950       3582    3        NaN     NaN    NaN    NaN          1           1   \n",
       "3321       2253    5        NaN     NaN    NaN    NaN          1           0   \n",
       "3777       5106    8        NaN     NaN    NaN    NaN          1           1   \n",
       "4036       5896    9        NaN     NaN    NaN    NaN          1           0   \n",
       "4421       8043    7        NaN     NaN    NaN    NaN          1           0   \n",
       "4461       1965    5        NaN     NaN    NaN    NaN          1           0   \n",
       "4722        344    8        NaN     NaN    NaN    NaN          1           1   \n",
       "4860       7198    2        NaN     NaN    NaN    NaN          0           0   \n",
       "4960       4485    5        NaN     NaN    NaN    NaN          0           0   \n",
       "5125       8007    5        NaN     NaN    NaN    NaN          1           0   \n",
       "\n",
       "      radio  cooker    ...     ModCon  usecondom  hivknow  lowlit  highlit  \\\n",
       "323       0       0    ...        0.0        0.0      0.0     NaN      NaN   \n",
       "453       1       0    ...        0.0        0.0      1.0     1.0      0.0   \n",
       "685       0       0    ...        1.0        0.0      0.0     NaN      NaN   \n",
       "1191      1       0    ...        0.0        NaN      0.0     0.0      0.0   \n",
       "1887      1       0    ...        0.0        NaN      0.0     1.0      0.0   \n",
       "1950      0       0    ...        1.0        0.0      1.0     0.0      0.0   \n",
       "3321      1       0    ...        0.0        NaN      0.0     0.0      1.0   \n",
       "3777      0       0    ...        0.0        NaN      0.0     0.0      0.0   \n",
       "4036      1       0    ...        1.0        NaN      0.0     1.0      0.0   \n",
       "4421      0       0    ...        0.0        NaN      NaN     0.0      0.0   \n",
       "4461      1       0    ...        1.0        0.0      1.0     0.0      0.0   \n",
       "4722      0       0    ...        0.0        0.0      0.0     0.0      0.0   \n",
       "4860      0       0    ...        0.0        0.0      0.0     NaN      NaN   \n",
       "4960      1       0    ...        0.0        NaN      0.0     0.0      0.0   \n",
       "5125      0       0    ...        NaN        0.0      1.0     1.0      0.0   \n",
       "\n",
       "      urban  rural  single  segment  subgroup  \n",
       "323       0      1     0.0        3         1  \n",
       "453       0      1     0.0        4         1  \n",
       "685       1      0     0.0        3         1  \n",
       "1191      0      1     0.0        2         1  \n",
       "1887      1      0     0.0        2         1  \n",
       "1950      1      0     0.0        2         2  \n",
       "3321      0      1     0.0        1         2  \n",
       "3777      0      1     NaN        4         1  \n",
       "4036      0      1     0.0        1         1  \n",
       "4421      0      0     NaN        2         1  \n",
       "4461      1      0     0.0        3         1  \n",
       "4722      0      0     0.0        4         1  \n",
       "4860      0      1     0.0        1         2  \n",
       "4960      1      0     NaN        1         1  \n",
       "5125      0      1     0.0        3         1  \n",
       "\n",
       "[15 rows x 50 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[np.isnan(df['christian'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dataset , the religion has been separated into various religions within the datset so there is no need to change the string variable to numerical values. Also, Religion has only 15 rows missing from the whole dataset , I will drop these rows as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.drop('religion', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset = ['christian'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the assumptions for the missing data depending on the competition about sex and health.\n",
    "'married has 22 missing : Yes', \n",
    "'multpart has 1154 missing : 1', \n",
    "'educ has 90 missing : Median ', \n",
    "'inschool has 17 missing: Yes', \n",
    "'ownincome has 59 missing : No', \n",
    "'literacy has 113 missing : Median', \n",
    "'LaborDeliv has 2513 missing : No',\n",
    "'babydoc has 67 missing : Median', \n",
    "'Debut has 566 missing : Median', \n",
    "'ModCon has 318 missing : No', \n",
    "'usecondom has 2360 missing : No', \n",
    "'hivknow has 607 missing : No', \n",
    "'lowlit has 113 missing : Median', \n",
    "'highlit has 113 missing : Median', \n",
    "'single has 1383 missing : No'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['married'].fillna(1, inplace=True)\n",
    "df['multpart'].fillna(1, inplace=True)\n",
    "df['educ'].fillna(df['educ'].median(), inplace=True)\n",
    "df['inschool'].fillna(1, inplace=True)\n",
    "df['ownincome'].fillna(0, inplace=True)\n",
    "df['literacy'].fillna(df['literacy'].median(), inplace=True)\n",
    "df['LaborDeliv'].fillna(0, inplace=True)\n",
    "df['babydoc'].fillna(df['babydoc'].median(), inplace=True)\n",
    "df['Debut'].fillna(df['Debut'].median(), inplace=True)\n",
    "df['ModCon'].fillna(0, inplace=True)\n",
    "df['usecondom'].fillna(0, inplace=True)\n",
    "df['hivknow'].fillna(0, inplace=True)\n",
    "df['lowlit'].fillna(df['lowlit'].median(), inplace=True)\n",
    "df['highlit'].fillna(df['highlit'].median(), inplace=True)\n",
    "df['single'].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We will work with this dataset in all examples, namely, with the X feature-object matrix and values of the y target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate the data from the target attributes\n",
    "X = df[['patientID', 'christian', 'muslim', 'hindu', 'other',\n",
    "       'cellphone', 'motorcycle', 'radio', 'cooker', 'fridge', 'furniture',\n",
    "       'computer', 'cart', 'irrigation', 'thrasher', 'car', 'generator',\n",
    "       'INTNR', 'REGION_PROVINCE', 'DISTRICT', 'electricity', 'age', 'tribe',\n",
    "       'foodinsecurity', 'EVER_HAD_SEX', 'EVER_BEEN_PREGNANT', 'CHILDREN',\n",
    "       'india', 'married', 'multpart', 'educ', 'inschool', 'ownincome',\n",
    "       'literacy', 'urbanicity', 'LaborDeliv', 'babydoc', 'Debut', 'ModCon',\n",
    "       'usecondom', 'hivknow', 'lowlit', 'highlit', 'urban', 'rural', 'single']]\n",
    "y = df[['geo','segment','subgroup']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of us know well that the majority of gradient methods (on which almost all machine learning algorithms are based) are highly sensitive to data scaling. Therefore, before running an algorithm, we should perform either normalization, or the so-called standardization. Normalization involves replacing nominal features, so that each of them would be in the range from 0 to 1. As for standardization, it involves data pre-processing, after which each feature has an average 0 and 1 dispersion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# normalize the data attributes\n",
    "normalized_X = preprocessing.normalize(X)\n",
    "# standardize the data attributes\n",
    "standardized_X = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html\n",
    "http://stackoverflow.com/questions/34012100/scikit-learn-bad-input-shape-error-on-multinomial-logistic-regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### I will group all the three variables into one total for predicting the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, col in enumerate(y.columns.tolist(), 1):\n",
    "    y.loc[:, col] *= i\n",
    "y = y.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "# create the RFE model and select 3 attributes\n",
    "rfe = RFE(model, 3)\n",
    "rfe = rfe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summarize the selection of the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False False\n",
      " False False False False False False  True False False  True False False\n",
      " False False False False False False False False False  True False False\n",
      " False False False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "print(rfe.support_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14 15 33 29 41 22 35 18 32 36 28 40 37 43 38 42 39  5  1  4  8  1  2  6 10\n",
      " 11 12 27 13  9  7 24 20  1 16 17 21  3 23 30 19 31 34 26 25 44]\n"
     ]
    }
   ],
   "source": [
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most often used for solving tasks of classification (binary), but multiclass classification (the so-called one-vs-all method) is also allowed. The advantage of this algorithm is that thereâ€™s the probability of belonging to a class for each object at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model.stochastic_gradient import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.675398633257\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(standardized_X, y)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = y\n",
    "predicted = model.predict(standardized_X)\n",
    "# summarize the fit of the model\n",
    "#print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.accuracy_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SGDClassier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to discriminative learning of linear classifiers under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
      "       penalty='l2', power_t=0.5, random_state=1, shuffle=True, verbose=0,\n",
      "       warm_start=False)\n",
      "0.526195899772\n"
     ]
    }
   ],
   "source": [
    "model = SGDClassifier(random_state=1)\n",
    "model.fit(standardized_X, y)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = y\n",
    "predicted = model.predict(standardized_X)\n",
    "# summarize the fit of the model\n",
    "#print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.accuracy_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "\n",
    "Is also one of the most well-known machine learning algorithms, the main task of which is to restore the density of data distribution of the training sample. This method often provides good quality in multiclass classification problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "0.276006074412\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(standardized_X, y)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = y\n",
    "predicted = model.predict(standardized_X)\n",
    "# summarize the fit of the model\n",
    "#print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.accuracy_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-Nearest Neighbours\n",
    "The kNN (k-Nearest Neighbors) method is often used as part of a more complex classification algorithm. For instance, we can use its estimate as an objectâ€™s feature. Sometimes, a simple kNN provides great quality on well-chosen features. When parameters (metrics mostly) are set well, the algorithm often gives good quality in regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "0.739749430524\n"
     ]
    }
   ],
   "source": [
    "# fit a k-nearest neighbor model to the data\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(standardized_X, y)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = y\n",
    "predicted = model.predict(standardized_X)\n",
    "# summarize the fit of the model\n",
    "#print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.accuracy_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees\n",
    "Classification and Regression Trees (CART) are often used in problems, in which objects have category features and used for regression and classification problems. The trees are very well suited for multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# fit a CART model to the data\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(standardized_X, y)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = y\n",
    "predicted = model.predict(standardized_X)\n",
    "# summarize the fit of the model\n",
    "#print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.accuracy_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines\n",
    "SVM (Support Vector Machines) is one of the most popular machine learning algorithms used mainly for the classification problem. As well as logistic regression, SVM allows multi-class classification with the help of the one-vs-all method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "0.836750189825\n"
     ]
    }
   ],
   "source": [
    "# fit a SVM model to the data\n",
    "model = SVC()\n",
    "model.fit(standardized_X, y)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = y\n",
    "predicted = model.predict(standardized_X)\n",
    "# summarize the fit of the model\n",
    "#print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.accuracy_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Algorithm Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "letâ€™s take a look at the selection of the regularization parameter, in which several values are searched in turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'alpha': array([  1.00000e+00,   1.00000e-01,   1.00000e-02,   1.00000e-03,\n",
      "         1.00000e-04,   0.00000e+00])},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)\n",
      "0.425103400262\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "# prepare a range of alpha values to test\n",
    "alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "# create and fit a ridge regression model, testing each alpha\n",
    "model = Ridge()\n",
    "grid = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))\n",
    "grid.fit(X, y)\n",
    "print(grid)\n",
    "# summarize the results of the grid search\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is more efficient to randomly select a parameter from the given range, estimate the algorithm quality for this parameter and choose the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV(cv=None, error_score='raise',\n",
      "          estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
      "          fit_params={}, iid=True, n_iter=100, n_jobs=1,\n",
      "          param_distributions={'alpha': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000020B1775FBA8>},\n",
      "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
      "          scoring=None, verbose=0)\n",
      "0.425095237385\n",
      "0.9958047356368019\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "# prepare a uniform distribution to sample for the alpha parameter\n",
    "param_grid = {'alpha': sp_rand()}\n",
    "# create and fit a ridge regression model, testing random alpha values\n",
    "model = Ridge()\n",
    "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100)\n",
    "rsearch.fit(standardized_X, y)\n",
    "print(rsearch)\n",
    "# summarize the results of the random parameter search\n",
    "print(rsearch.best_score_)\n",
    "print(rsearch.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
