{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import datetime\n",
    "pd.set_option('display.max_columns', None)\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "from operator import itemgetter\n",
    "import time\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Function to Write result in csv file to submit \n",
    "###########################################################################\n",
    "\n",
    "def write_to_csv(output,score):\n",
    "    now = datetime.datetime.now()\n",
    "    sub_file = 'submission_' + str(score) + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "    print('Writing submission: ', sub_file)\n",
    "    f = open(sub_file, 'w')\n",
    "    prediction_file_object = csv.writer(f)\n",
    "    prediction_file_object.writerow([\"Id\",\"SalePrice\"])  # don't forget the headers\n",
    "\n",
    "    for i in range(len(test)):\n",
    "        prediction_file_object.writerow([test[\"Id\"][test.index[i]], (output[i])])\n",
    "\n",
    "\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Function to process features \n",
    "###########################################################################\n",
    "def get_features(train, test):\n",
    "    trainval = list(train.columns.values) # list train features\n",
    "    testval = list(test.columns.values) # list test features\n",
    "    output = list(set(trainval) & set(testval)) # check wich features are in common (remove the outcome column)\n",
    "    output.remove('Id') # remove non-usefull id column\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def process_features(train,test):\n",
    "    tables=[test,train]\n",
    "    print (\"Handling missing values...\")\n",
    "    total_missing=train.isnull().sum()\n",
    "    to_delete=total_missing[total_missing>(1460/3.)] # select features with more than 1/3 missing values\n",
    "    for table in tables:\n",
    "        table.drop(to_delete.index.tolist(),axis=1, inplace=True)\n",
    "            \n",
    "    print (\"Filling Nan...\")\n",
    "    numerical_features=test.select_dtypes(include=[\"float\",\"int\",\"bool\"]).columns.values\n",
    "    categorical_features=train.select_dtypes(include=[\"object\"]).columns.values\n",
    "    for table in tables: \n",
    "        for feature in numerical_features: \n",
    "            table[feature].fillna(train[feature].median(), inplace = True) # replace by median value\n",
    "        for feature in categorical_features: \n",
    "            table[feature].fillna(train[feature].value_counts().idxmax(), inplace = True) # replace by most frequent value\n",
    "\n",
    "    print (\"Handling categorical features...\")\n",
    "    for feature in categorical_features: # Encode categorical features\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(train[feature])\n",
    "        for table in tables: \n",
    "            table[feature]=le.transform(table[feature])\n",
    "    \n",
    "    print (\"Getting features...\")\n",
    "    features = get_features(train,test)\n",
    "    \n",
    "    return train,test,features\n",
    "\n",
    "def train_and_test_linear(train,test,features,target='SalePrice'): # simple xgboost\n",
    "    subsample = 0.8\n",
    "    colsample_bytree = 0.8\n",
    "    num_boost_round = 1000 #115 originally \n",
    "    early_stopping_rounds = 50\n",
    "    test_size = 0.2 # 0.1 originally\n",
    "    \n",
    "    # start the training\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"reg:linear\",\n",
    "        \"booster\" : \"gblinear\", #\"gbtree\",# default\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"subsample\": subsample, # collect 80% of the data only to prevent overfitting\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\": 1,\n",
    "        \"seed\": 0,\n",
    "    }\n",
    "    X_train, X_valid = train_test_split(train, test_size=test_size, random_state=0) # randomly split into 90% test and 10% CV -> still has the outcome at this point\n",
    "    y_train = np.log(X_train[target]) # define y as the outcome column, apply log to have same error as the leaderboard\n",
    "    y_valid = np.log(X_valid[target])\n",
    "    dtrain = xgb.DMatrix(X_train[features], y_train) # DMatrix are matrix for xgboost\n",
    "    dvalid = xgb.DMatrix(X_valid[features], y_valid)\n",
    "\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')] # list of things to evaluate and print\n",
    "    gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=True) # find the best score\n",
    "    score = gbm.best_score #roc_auc_score(X_valid[target].values, check)\n",
    "    print('Last error value: {:.6f}'.format(score))\n",
    "\n",
    "    print(\"Predict test set...\")\n",
    "    test_prediction = gbm.predict(xgb.DMatrix(test[features]))\n",
    "\n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "\n",
    "    return test_prediction, score \n",
    "def train_and_test_tree(train,test,features,target='SalePrice'): # simple xgboost\n",
    "    eta_list = [0.1,0.2] # list of parameters to try\n",
    "    max_depth_list = [4,6,8] # list of parameters to try\n",
    "    subsample = 0.8\n",
    "    colsample_bytree = 0.8\n",
    "    \n",
    "    num_boost_round = 400 \n",
    "    early_stopping_rounds = 10\n",
    "    test_size = 0.2 \n",
    "    \n",
    "    start_time = time.time()\n",
    "   \n",
    "    # start the training\n",
    "    array_score=np.ndarray((len(eta_list)*len(max_depth_list),3)) # store score values\n",
    "    i=0\n",
    "    for eta,max_depth in list(itertools.product(eta_list, max_depth_list)): # Loop over parameters to find the better set\n",
    "        print('XGBoost params. ETA: {}, MAX_DEPTH: {}, SUBSAMPLE: {}, COLSAMPLE_BY_TREE: {}'.format(eta, max_depth, subsample, colsample_bytree))\n",
    "        params = {\n",
    "            \"objective\": \"reg:linear\",\n",
    "            \"booster\" : \"gbtree\", \n",
    "            \"eval_metric\": \"rmse\", # this is the metric for the leardboard\n",
    "            \"eta\": eta, # shrinking parameters to prevent overfitting\n",
    "            \"tree_method\": 'exact',\n",
    "            \"max_depth\": max_depth,\n",
    "            \"subsample\": subsample, # collect 80% of the data only to prevent overfitting\n",
    "            \"colsample_bytree\": colsample_bytree,\n",
    "            \"silent\": 1,\n",
    "            \"seed\": 0,\n",
    "        }\n",
    "    \n",
    "        X_train, X_valid = train_test_split(train, test_size=test_size, random_state=0) # randomly split into 90% test and 10% CV -> still has the outcome at this point\n",
    "        y_train = np.log(X_train[target]) # define y as the outcome column\n",
    "        y_valid = np.log(X_valid[target])\n",
    "        dtrain = xgb.DMatrix(X_train[features], y_train) # DMatrix are matrix for xgboost\n",
    "        dvalid = xgb.DMatrix(X_valid[features], y_valid)\n",
    "\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')] # list of things to evaluate and print\n",
    "        gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=True) # find the best score\n",
    "\n",
    "        print(\"Validating...\")\n",
    "        score = gbm.best_score \n",
    "        print('Last error value: {:.6f}'.format(score))\n",
    "        array_score[i][0]=eta\n",
    "        array_score[i][1]=max_depth\n",
    "        array_score[i][2]=score\n",
    "        i+=1\n",
    "    df_score=pd.DataFrame(array_score,columns=['eta','max_depth','SalePrice'])\n",
    "    print(\"df_score : \\n\", df_score)\n",
    "    #create_feature_map(features)\n",
    "    importance = gbm.get_fscore()\n",
    "    importance = sorted(importance.items(), key=itemgetter(1), reverse=True)\n",
    "    print('Importance array: ', importance)\n",
    "    np.save(\"features_importance\",importance) # save feature importance for latter use\n",
    "    print(\"Predict test set...\")\n",
    "    test_prediction = gbm.predict(xgb.DMatrix(test[features]), ntree_limit=gbm.best_ntree_limit) # only predict with the last set of parameters\n",
    "\n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "\n",
    "    return test_prediction, score \n",
    "\n",
    "def train_and_test_Kfold(train,test,features,target='SalePrice'): # add Kfold\n",
    "    eta_list = [0.02] # list of parameters to try\n",
    "    max_depth_list = [6]\n",
    "    subsample = 1 # No subsampling, as we already use Kfold latter and we don't have that much data\n",
    "    colsample_bytree = 1\n",
    "    \n",
    "    num_boost_round = 5000 # for small eta, increase this one\n",
    "    early_stopping_rounds = 500\n",
    "    n_folds=3 \n",
    "    start_time = time.time()\n",
    "   \n",
    "\n",
    "    # start the training\n",
    "    array_score=np.ndarray((len(eta_list)*len(max_depth_list),4)) # store score values\n",
    "    i=0\n",
    "    for eta,max_depth in list(itertools.product(eta_list, max_depth_list)): # Loop over parameters to find the better set\n",
    "        print('XGBoost params. ETA: {}, MAX_DEPTH: {}'.format(eta, max_depth))\n",
    "        params = {\n",
    "            \"objective\": \"reg:linear\",\n",
    "            \"booster\" : \"gbtree\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"eta\": eta, # shrinking parameters to prevent overfitting\n",
    "            \"tree_method\": 'exact',\n",
    "            \"max_depth\": max_depth,\n",
    "            \"subsample\": subsample, # collect 80% of the data only to prevent overfitting\n",
    "            \"colsample_bytree\": colsample_bytree,\n",
    "            \"silent\": 1,\n",
    "            \"seed\": 0,\n",
    "        }\n",
    "        kf = KFold(len(train), n_folds=n_folds)\n",
    "        test_prediction=np.ndarray((n_folds,len(test)))\n",
    "        fold=0\n",
    "        fold_score=[]\n",
    "        for train_index, cv_index in kf:\n",
    "            X_train, X_valid    = train[features].as_matrix()[train_index], train[features].as_matrix()[cv_index]\n",
    "            y_train, y_valid    = np.log(train[target].as_matrix()[train_index]), np.log(train[target].as_matrix()[cv_index])\n",
    "\n",
    "            dtrain = xgb.DMatrix(X_train, y_train) # DMatrix are matrix for xgboost\n",
    "            dvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "\n",
    "            watchlist = [(dtrain, 'train'), (dvalid, 'eval')] # list of things to evaluate and print\n",
    "            gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=True) # find the best score\n",
    "\n",
    "            print(\"Validating...\")\n",
    "            check = gbm.predict(xgb.DMatrix(X_valid)) # get the best score\n",
    "            score = gbm.best_score\n",
    "            print('Check last score value: {:.6f}'.format(score))\n",
    "            fold_score.append(score)\n",
    "            importance = gbm.get_fscore()\n",
    "            importance = sorted(importance.items(), key=itemgetter(1), reverse=True)\n",
    "            print('Importance array for fold {} :\\n {}'.format(fold, importance))\n",
    "            #np.save(\"features_importance\",importance)\n",
    "            print(\"Predict test set...\")\n",
    "            prediction=gbm.predict(xgb.DMatrix(test[features].as_matrix()))\n",
    "            #np.save(\"prediction_eta%s_depth%s_fold%s\" %(eta,max_depth,fold),prediction) # You can save all the folds prediction to check for errors in code\n",
    "            test_prediction[fold]=prediction\n",
    "            fold = fold + 1\n",
    "        mean_score=np.mean(fold_score)\n",
    "        print(\"Mean Score : {}, eta : {}, depth : {}\\n\".format(mean_score,eta,max_depth))\n",
    "        array_score[i][0]=eta\n",
    "        array_score[i][1]=max_depth\n",
    "        array_score[i][2]=mean_score\n",
    "        array_score[i][3]=np.std(fold_score)\n",
    "        i+=1\n",
    "    final_prediction=test_prediction.mean(axis=0)\n",
    "    df_score=pd.DataFrame(array_score,columns=['eta','max_depth','mean_score','std_score'])\n",
    "    print (\"df_score : \\n\", df_score)# get the complete array of scores to choose the right parameters\n",
    "\n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "\n",
    "    return final_prediction, mean_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing values...\n",
      "Filling Nan...\n",
      "Handling categorical features...\n",
      "Getting features...\n",
      "XGBoost params. ETA: 0.02, MAX_DEPTH: 6\n",
      "[0]\ttrain-rmse:11.2945\teval-rmse:11.314\n",
      "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmse hasn't improved in 500 rounds.\n",
      "[1]\ttrain-rmse:11.0691\teval-rmse:11.0886\n",
      "[2]\ttrain-rmse:10.8482\teval-rmse:10.8677\n",
      "[3]\ttrain-rmse:10.6318\teval-rmse:10.6513\n",
      "[4]\ttrain-rmse:10.4197\teval-rmse:10.4392\n",
      "[5]\ttrain-rmse:10.2118\teval-rmse:10.2313\n",
      "[6]\ttrain-rmse:10.0081\teval-rmse:10.0276\n",
      "[7]\ttrain-rmse:9.80845\teval-rmse:9.82797\n",
      "[8]\ttrain-rmse:9.61281\teval-rmse:9.63233\n",
      "[9]\ttrain-rmse:9.42108\teval-rmse:9.4406\n",
      "[10]\ttrain-rmse:9.23319\teval-rmse:9.25272\n",
      "[11]\ttrain-rmse:9.04907\teval-rmse:9.06859\n",
      "[12]\ttrain-rmse:8.86862\teval-rmse:8.88815\n",
      "[13]\ttrain-rmse:8.69179\teval-rmse:8.71133\n",
      "[14]\ttrain-rmse:8.5185\teval-rmse:8.53804\n",
      "[15]\ttrain-rmse:8.34868\teval-rmse:8.36822\n",
      "[16]\ttrain-rmse:8.18226\teval-rmse:8.20181\n",
      "[17]\ttrain-rmse:8.01917\teval-rmse:8.03851\n",
      "[18]\ttrain-rmse:7.85934\teval-rmse:7.87847\n",
      "[19]\ttrain-rmse:7.7027\teval-rmse:7.72163\n",
      "[20]\ttrain-rmse:7.5492\teval-rmse:7.56792\n",
      "[21]\ttrain-rmse:7.39876\teval-rmse:7.41728\n",
      "[22]\ttrain-rmse:7.25133\teval-rmse:7.26966\n",
      "[23]\ttrain-rmse:7.10684\teval-rmse:7.12499\n",
      "[24]\ttrain-rmse:6.96525\teval-rmse:6.98321\n",
      "[25]\ttrain-rmse:6.82648\teval-rmse:6.84426\n",
      "[26]\ttrain-rmse:6.6905\teval-rmse:6.7081\n",
      "[27]\ttrain-rmse:6.55723\teval-rmse:6.57465\n",
      "[28]\ttrain-rmse:6.42662\teval-rmse:6.44405\n",
      "[29]\ttrain-rmse:6.29862\teval-rmse:6.3157\n",
      "[30]\ttrain-rmse:6.17318\teval-rmse:6.19027\n",
      "[31]\ttrain-rmse:6.05025\teval-rmse:6.06717\n",
      "[32]\ttrain-rmse:5.92978\teval-rmse:5.94636\n",
      "[33]\ttrain-rmse:5.81171\teval-rmse:5.82831\n",
      "[34]\ttrain-rmse:5.69601\teval-rmse:5.71244\n",
      "[35]\ttrain-rmse:5.58262\teval-rmse:5.59906\n",
      "[36]\ttrain-rmse:5.47149\teval-rmse:5.48761\n",
      "[37]\ttrain-rmse:5.36259\teval-rmse:5.37855\n",
      "[38]\ttrain-rmse:5.25587\teval-rmse:5.27179\n",
      "[39]\ttrain-rmse:5.15128\teval-rmse:5.16715\n",
      "[40]\ttrain-rmse:5.04878\teval-rmse:5.06434\n",
      "[41]\ttrain-rmse:4.94833\teval-rmse:4.96374\n",
      "[42]\ttrain-rmse:4.84989\teval-rmse:4.86525\n",
      "[43]\ttrain-rmse:4.75342\teval-rmse:4.76891\n",
      "[44]\ttrain-rmse:4.65888\teval-rmse:4.67433\n",
      "[45]\ttrain-rmse:4.56623\teval-rmse:4.58163\n",
      "[46]\ttrain-rmse:4.47543\teval-rmse:4.49054\n",
      "[47]\ttrain-rmse:4.38645\teval-rmse:4.40134\n",
      "[48]\ttrain-rmse:4.29924\teval-rmse:4.31409\n",
      "[49]\ttrain-rmse:4.21379\teval-rmse:4.22859\n",
      "[50]\ttrain-rmse:4.13004\teval-rmse:4.14497\n",
      "[51]\ttrain-rmse:4.04797\teval-rmse:4.06268\n",
      "[52]\ttrain-rmse:3.96754\teval-rmse:3.98212\n",
      "[53]\ttrain-rmse:3.88872\teval-rmse:3.90312\n",
      "[54]\ttrain-rmse:3.81148\teval-rmse:3.82574\n",
      "[55]\ttrain-rmse:3.73577\teval-rmse:3.74987\n",
      "[56]\ttrain-rmse:3.66159\teval-rmse:3.67569\n",
      "[57]\ttrain-rmse:3.58889\teval-rmse:3.60296\n",
      "[58]\ttrain-rmse:3.51765\teval-rmse:3.5314\n",
      "[59]\ttrain-rmse:3.44782\teval-rmse:3.46141\n",
      "[60]\ttrain-rmse:3.3794\teval-rmse:3.39304\n",
      "[61]\ttrain-rmse:3.31234\teval-rmse:3.32577\n",
      "[62]\ttrain-rmse:3.24663\teval-rmse:3.26002\n",
      "[63]\ttrain-rmse:3.18223\teval-rmse:3.19567\n",
      "[64]\ttrain-rmse:3.11913\teval-rmse:3.13257\n",
      "[65]\ttrain-rmse:3.05728\teval-rmse:3.07044\n",
      "[66]\ttrain-rmse:2.99667\teval-rmse:3.00987\n",
      "[67]\ttrain-rmse:2.93728\teval-rmse:2.95011\n",
      "[68]\ttrain-rmse:2.87907\teval-rmse:2.89163\n",
      "[69]\ttrain-rmse:2.82202\teval-rmse:2.83444\n",
      "[70]\ttrain-rmse:2.76611\teval-rmse:2.77858\n",
      "[71]\ttrain-rmse:2.71132\teval-rmse:2.72383\n",
      "[72]\ttrain-rmse:2.65764\teval-rmse:2.66993\n",
      "[73]\ttrain-rmse:2.60503\teval-rmse:2.61698\n",
      "[74]\ttrain-rmse:2.55346\teval-rmse:2.56528\n",
      "[75]\ttrain-rmse:2.50292\teval-rmse:2.5148\n",
      "[76]\ttrain-rmse:2.4534\teval-rmse:2.4651\n",
      "[77]\ttrain-rmse:2.40487\teval-rmse:2.41656\n",
      "[78]\ttrain-rmse:2.35732\teval-rmse:2.36899\n",
      "[79]\ttrain-rmse:2.31072\teval-rmse:2.32228\n",
      "[80]\ttrain-rmse:2.26504\teval-rmse:2.27632\n",
      "[81]\ttrain-rmse:2.22029\teval-rmse:2.23142\n",
      "[82]\ttrain-rmse:2.17642\teval-rmse:2.18741\n",
      "[83]\ttrain-rmse:2.13343\teval-rmse:2.14447\n",
      "[84]\ttrain-rmse:2.0913\teval-rmse:2.10253\n",
      "[85]\ttrain-rmse:2.05002\teval-rmse:2.06106\n",
      "[86]\ttrain-rmse:2.00957\teval-rmse:2.02058\n",
      "[87]\ttrain-rmse:1.96992\teval-rmse:1.98086\n",
      "[88]\ttrain-rmse:1.93107\teval-rmse:1.94192\n",
      "[89]\ttrain-rmse:1.89299\teval-rmse:1.9036\n",
      "[90]\ttrain-rmse:1.85568\teval-rmse:1.86623\n",
      "[91]\ttrain-rmse:1.81912\teval-rmse:1.82943\n",
      "[92]\ttrain-rmse:1.78328\teval-rmse:1.79359\n",
      "[93]\ttrain-rmse:1.74817\teval-rmse:1.75838\n",
      "[94]\ttrain-rmse:1.71374\teval-rmse:1.72387\n",
      "[95]\ttrain-rmse:1.68002\teval-rmse:1.69015\n",
      "[96]\ttrain-rmse:1.64696\teval-rmse:1.65714\n",
      "[97]\ttrain-rmse:1.61457\teval-rmse:1.62489\n",
      "[98]\ttrain-rmse:1.58283\teval-rmse:1.59326\n",
      "[99]\ttrain-rmse:1.55172\teval-rmse:1.56207\n",
      "[100]\ttrain-rmse:1.52123\teval-rmse:1.53138\n",
      "[101]\ttrain-rmse:1.49137\teval-rmse:1.50145\n",
      "[102]\ttrain-rmse:1.4621\teval-rmse:1.47201\n",
      "[103]\ttrain-rmse:1.43341\teval-rmse:1.4434\n",
      "[104]\ttrain-rmse:1.40529\teval-rmse:1.41523\n",
      "[105]\ttrain-rmse:1.37774\teval-rmse:1.38765\n",
      "[106]\ttrain-rmse:1.35075\teval-rmse:1.36067\n",
      "[107]\ttrain-rmse:1.32429\teval-rmse:1.3341\n",
      "[108]\ttrain-rmse:1.29836\teval-rmse:1.30824\n",
      "[109]\ttrain-rmse:1.27293\teval-rmse:1.28274\n",
      "[110]\ttrain-rmse:1.24803\teval-rmse:1.25804\n",
      "[111]\ttrain-rmse:1.22364\teval-rmse:1.23366\n",
      "[112]\ttrain-rmse:1.19972\teval-rmse:1.20966\n",
      "[113]\ttrain-rmse:1.17629\teval-rmse:1.18619\n",
      "[114]\ttrain-rmse:1.15332\teval-rmse:1.16325\n",
      "[115]\ttrain-rmse:1.13082\teval-rmse:1.1407\n",
      "[116]\ttrain-rmse:1.10877\teval-rmse:1.11866\n",
      "[117]\ttrain-rmse:1.08715\teval-rmse:1.09695\n",
      "[118]\ttrain-rmse:1.06598\teval-rmse:1.07586\n",
      "[119]\ttrain-rmse:1.04523\teval-rmse:1.05509\n",
      "[120]\ttrain-rmse:1.02489\teval-rmse:1.03463\n",
      "[121]\ttrain-rmse:1.00496\teval-rmse:1.01476\n",
      "[122]\ttrain-rmse:0.985441\teval-rmse:0.995253\n",
      "[123]\ttrain-rmse:0.966299\teval-rmse:0.976187\n",
      "[124]\ttrain-rmse:0.947548\teval-rmse:0.957454\n",
      "[125]\ttrain-rmse:0.929178\teval-rmse:0.939026\n",
      "[126]\ttrain-rmse:0.911176\teval-rmse:0.921109\n",
      "[127]\ttrain-rmse:0.89353\teval-rmse:0.903593\n",
      "[128]\ttrain-rmse:0.876225\teval-rmse:0.886429\n",
      "[129]\ttrain-rmse:0.859282\teval-rmse:0.869662\n",
      "[130]\ttrain-rmse:0.84266\teval-rmse:0.853094\n",
      "[131]\ttrain-rmse:0.826389\teval-rmse:0.83685\n",
      "[132]\ttrain-rmse:0.810441\teval-rmse:0.82096\n",
      "[133]\ttrain-rmse:0.7948\teval-rmse:0.805464\n",
      "[134]\ttrain-rmse:0.779474\teval-rmse:0.790161\n",
      "[135]\ttrain-rmse:0.764462\teval-rmse:0.775282\n",
      "[136]\ttrain-rmse:0.749743\teval-rmse:0.76066\n",
      "[137]\ttrain-rmse:0.735338\teval-rmse:0.746311\n",
      "[138]\ttrain-rmse:0.721218\teval-rmse:0.732237\n",
      "[139]\ttrain-rmse:0.707367\teval-rmse:0.718516\n",
      "[140]\ttrain-rmse:0.693811\teval-rmse:0.705065\n",
      "[141]\ttrain-rmse:0.680519\teval-rmse:0.691914\n",
      "[142]\ttrain-rmse:0.667495\teval-rmse:0.678956\n",
      "[143]\ttrain-rmse:0.654744\teval-rmse:0.666285\n",
      "[144]\ttrain-rmse:0.642245\teval-rmse:0.65386\n",
      "[145]\ttrain-rmse:0.630005\teval-rmse:0.641667\n",
      "[146]\ttrain-rmse:0.618004\teval-rmse:0.629865\n",
      "[147]\ttrain-rmse:0.606247\teval-rmse:0.618141\n",
      "[148]\ttrain-rmse:0.59472\teval-rmse:0.606679\n",
      "[149]\ttrain-rmse:0.583431\teval-rmse:0.595485\n",
      "[150]\ttrain-rmse:0.572354\teval-rmse:0.58443\n",
      "[151]\ttrain-rmse:0.561505\teval-rmse:0.573669\n",
      "[152]\ttrain-rmse:0.550877\teval-rmse:0.563225\n",
      "[153]\ttrain-rmse:0.540452\teval-rmse:0.552962\n",
      "[154]\ttrain-rmse:0.530233\teval-rmse:0.542849\n",
      "[155]\ttrain-rmse:0.520231\teval-rmse:0.532996\n",
      "[156]\ttrain-rmse:0.510424\teval-rmse:0.523289\n",
      "[157]\ttrain-rmse:0.500812\teval-rmse:0.513761\n",
      "[158]\ttrain-rmse:0.491398\teval-rmse:0.50445\n",
      "[159]\ttrain-rmse:0.482172\teval-rmse:0.495373\n",
      "[160]\ttrain-rmse:0.473148\teval-rmse:0.486539\n",
      "[161]\ttrain-rmse:0.464295\teval-rmse:0.477762\n",
      "[162]\ttrain-rmse:0.455616\teval-rmse:0.469223\n",
      "[163]\ttrain-rmse:0.44712\teval-rmse:0.460809\n",
      "[164]\ttrain-rmse:0.43879\teval-rmse:0.452635\n",
      "[165]\ttrain-rmse:0.430629\teval-rmse:0.444611\n",
      "[166]\ttrain-rmse:0.422646\teval-rmse:0.436924\n",
      "[167]\ttrain-rmse:0.414805\teval-rmse:0.429286\n",
      "[168]\ttrain-rmse:0.407138\teval-rmse:0.421825\n",
      "[169]\ttrain-rmse:0.399628\teval-rmse:0.414504\n",
      "[170]\ttrain-rmse:0.392243\teval-rmse:0.407226\n",
      "[171]\ttrain-rmse:0.38501\teval-rmse:0.400215\n",
      "[172]\ttrain-rmse:0.377943\teval-rmse:0.39327\n",
      "[173]\ttrain-rmse:0.371011\teval-rmse:0.386577\n",
      "[174]\ttrain-rmse:0.364212\teval-rmse:0.379988\n",
      "[175]\ttrain-rmse:0.357558\teval-rmse:0.37356\n",
      "[176]\ttrain-rmse:0.351036\teval-rmse:0.367163\n",
      "[177]\ttrain-rmse:0.34464\teval-rmse:0.360899\n",
      "[178]\ttrain-rmse:0.338387\teval-rmse:0.354954\n",
      "[179]\ttrain-rmse:0.33226\teval-rmse:0.348963\n",
      "[180]\ttrain-rmse:0.32625\teval-rmse:0.343119\n",
      "[181]\ttrain-rmse:0.320378\teval-rmse:0.337534\n",
      "[182]\ttrain-rmse:0.314637\teval-rmse:0.332035\n",
      "[183]\ttrain-rmse:0.30899\teval-rmse:0.326682\n",
      "[184]\ttrain-rmse:0.303455\teval-rmse:0.321348\n",
      "[185]\ttrain-rmse:0.298048\teval-rmse:0.316123\n",
      "[186]\ttrain-rmse:0.292742\teval-rmse:0.311022\n",
      "[187]\ttrain-rmse:0.287548\teval-rmse:0.306119\n",
      "[188]\ttrain-rmse:0.282435\teval-rmse:0.301265\n",
      "[189]\ttrain-rmse:0.27746\teval-rmse:0.296577\n",
      "[190]\ttrain-rmse:0.272604\teval-rmse:0.291994\n",
      "[191]\ttrain-rmse:0.267851\teval-rmse:0.287494\n",
      "[192]\ttrain-rmse:0.263171\teval-rmse:0.283179\n",
      "[193]\ttrain-rmse:0.258598\teval-rmse:0.278876\n",
      "[194]\ttrain-rmse:0.254098\teval-rmse:0.274633\n",
      "[195]\ttrain-rmse:0.249663\teval-rmse:0.270536\n",
      "[196]\ttrain-rmse:0.245356\teval-rmse:0.266599\n",
      "[197]\ttrain-rmse:0.241166\teval-rmse:0.262723\n",
      "[198]\ttrain-rmse:0.237067\teval-rmse:0.25898\n",
      "[199]\ttrain-rmse:0.233034\teval-rmse:0.255252\n",
      "[200]\ttrain-rmse:0.229025\teval-rmse:0.251597\n",
      "[201]\ttrain-rmse:0.225158\teval-rmse:0.248\n",
      "[202]\ttrain-rmse:0.221358\teval-rmse:0.244499\n",
      "[203]\ttrain-rmse:0.217649\teval-rmse:0.241171\n",
      "[204]\ttrain-rmse:0.21402\teval-rmse:0.237964\n",
      "[205]\ttrain-rmse:0.210469\teval-rmse:0.234768\n",
      "[206]\ttrain-rmse:0.206934\teval-rmse:0.231667\n",
      "[207]\ttrain-rmse:0.203555\teval-rmse:0.228688\n",
      "[208]\ttrain-rmse:0.20023\teval-rmse:0.225739\n",
      "[209]\ttrain-rmse:0.196971\teval-rmse:0.222894\n",
      "[210]\ttrain-rmse:0.193765\teval-rmse:0.219948\n",
      "[211]\ttrain-rmse:0.190624\teval-rmse:0.21717\n",
      "[212]\ttrain-rmse:0.18757\teval-rmse:0.21448\n",
      "[213]\ttrain-rmse:0.184589\teval-rmse:0.211749\n",
      "[214]\ttrain-rmse:0.181679\teval-rmse:0.209278\n",
      "[215]\ttrain-rmse:0.17881\teval-rmse:0.206778\n",
      "[216]\ttrain-rmse:0.176001\teval-rmse:0.204306\n",
      "[217]\ttrain-rmse:0.17324\teval-rmse:0.201899\n",
      "[218]\ttrain-rmse:0.170536\teval-rmse:0.199538\n",
      "[219]\ttrain-rmse:0.167901\teval-rmse:0.197398\n",
      "[220]\ttrain-rmse:0.165332\teval-rmse:0.195213\n",
      "[221]\ttrain-rmse:0.162813\teval-rmse:0.193158\n",
      "[222]\ttrain-rmse:0.160337\teval-rmse:0.191115\n",
      "[223]\ttrain-rmse:0.157915\teval-rmse:0.189059\n",
      "[224]\ttrain-rmse:0.155546\teval-rmse:0.187108\n",
      "[225]\ttrain-rmse:0.153227\teval-rmse:0.185242\n",
      "[226]\ttrain-rmse:0.150995\teval-rmse:0.183401\n",
      "[227]\ttrain-rmse:0.148777\teval-rmse:0.181641\n",
      "[228]\ttrain-rmse:0.146589\teval-rmse:0.179897\n",
      "[229]\ttrain-rmse:0.144476\teval-rmse:0.178242\n",
      "[230]\ttrain-rmse:0.142387\teval-rmse:0.176551\n",
      "[231]\ttrain-rmse:0.140391\teval-rmse:0.174919\n",
      "[232]\ttrain-rmse:0.138414\teval-rmse:0.173386\n",
      "[233]\ttrain-rmse:0.136421\teval-rmse:0.171846\n",
      "[234]\ttrain-rmse:0.134506\teval-rmse:0.170373\n",
      "[235]\ttrain-rmse:0.132604\teval-rmse:0.168922\n",
      "[236]\ttrain-rmse:0.130807\teval-rmse:0.167561\n",
      "[237]\ttrain-rmse:0.128992\teval-rmse:0.166185\n",
      "[238]\ttrain-rmse:0.127276\teval-rmse:0.164831\n",
      "[239]\ttrain-rmse:0.125558\teval-rmse:0.163399\n",
      "[240]\ttrain-rmse:0.123923\teval-rmse:0.162135\n",
      "[241]\ttrain-rmse:0.122279\teval-rmse:0.160916\n",
      "[242]\ttrain-rmse:0.120639\teval-rmse:0.159668\n",
      "[243]\ttrain-rmse:0.119067\teval-rmse:0.158388\n",
      "[244]\ttrain-rmse:0.117575\teval-rmse:0.157267\n",
      "[245]\ttrain-rmse:0.116065\teval-rmse:0.156189\n",
      "[246]\ttrain-rmse:0.114589\teval-rmse:0.155172\n",
      "[247]\ttrain-rmse:0.1132\teval-rmse:0.154167\n",
      "[248]\ttrain-rmse:0.111825\teval-rmse:0.15317\n",
      "[249]\ttrain-rmse:0.110493\teval-rmse:0.152301\n",
      "[250]\ttrain-rmse:0.109171\teval-rmse:0.151433\n",
      "[251]\ttrain-rmse:0.107902\teval-rmse:0.150477\n",
      "[252]\ttrain-rmse:0.106593\teval-rmse:0.149559\n",
      "[253]\ttrain-rmse:0.105338\teval-rmse:0.148583\n",
      "[254]\ttrain-rmse:0.104136\teval-rmse:0.147794\n",
      "[255]\ttrain-rmse:0.102911\teval-rmse:0.146953\n",
      "[256]\ttrain-rmse:0.101729\teval-rmse:0.14615\n",
      "[257]\ttrain-rmse:0.100594\teval-rmse:0.145363\n",
      "[258]\ttrain-rmse:0.099404\teval-rmse:0.144621\n",
      "[259]\ttrain-rmse:0.098297\teval-rmse:0.143857\n",
      "[260]\ttrain-rmse:0.097162\teval-rmse:0.143151\n",
      "[261]\ttrain-rmse:0.096127\teval-rmse:0.142434\n",
      "[262]\ttrain-rmse:0.095045\teval-rmse:0.141776\n",
      "[263]\ttrain-rmse:0.09402\teval-rmse:0.141094\n",
      "[264]\ttrain-rmse:0.093081\teval-rmse:0.140515\n",
      "[265]\ttrain-rmse:0.092141\teval-rmse:0.139879\n",
      "[266]\ttrain-rmse:0.091173\teval-rmse:0.139259\n",
      "[267]\ttrain-rmse:0.090279\teval-rmse:0.138671\n",
      "[268]\ttrain-rmse:0.089344\teval-rmse:0.138115\n",
      "[269]\ttrain-rmse:0.088514\teval-rmse:0.137598\n",
      "[270]\ttrain-rmse:0.087682\teval-rmse:0.137061\n",
      "[271]\ttrain-rmse:0.086812\teval-rmse:0.136516\n",
      "[272]\ttrain-rmse:0.086049\teval-rmse:0.136013\n",
      "[273]\ttrain-rmse:0.085308\teval-rmse:0.135566\n",
      "[274]\ttrain-rmse:0.084597\teval-rmse:0.135135\n",
      "[275]\ttrain-rmse:0.083877\teval-rmse:0.134688\n",
      "[276]\ttrain-rmse:0.083203\teval-rmse:0.134285\n",
      "[277]\ttrain-rmse:0.08243\teval-rmse:0.13386\n",
      "[278]\ttrain-rmse:0.081788\teval-rmse:0.133474\n",
      "[279]\ttrain-rmse:0.081052\teval-rmse:0.13306\n",
      "[280]\ttrain-rmse:0.08047\teval-rmse:0.132705\n",
      "[281]\ttrain-rmse:0.079894\teval-rmse:0.132348\n",
      "[282]\ttrain-rmse:0.079218\teval-rmse:0.131999\n",
      "[283]\ttrain-rmse:0.078513\teval-rmse:0.131626\n",
      "[284]\ttrain-rmse:0.077943\teval-rmse:0.131273\n",
      "[285]\ttrain-rmse:0.077419\teval-rmse:0.130969\n",
      "[286]\ttrain-rmse:0.076863\teval-rmse:0.130624\n",
      "[287]\ttrain-rmse:0.076233\teval-rmse:0.130251\n",
      "[288]\ttrain-rmse:0.075734\teval-rmse:0.129982\n",
      "[289]\ttrain-rmse:0.075206\teval-rmse:0.129665\n",
      "[290]\ttrain-rmse:0.074747\teval-rmse:0.129399\n",
      "[291]\ttrain-rmse:0.074165\teval-rmse:0.129128\n",
      "[292]\ttrain-rmse:0.073738\teval-rmse:0.128888\n",
      "[293]\ttrain-rmse:0.07314\teval-rmse:0.1286\n",
      "[294]\ttrain-rmse:0.072697\teval-rmse:0.128368\n",
      "[295]\ttrain-rmse:0.07225\teval-rmse:0.128114\n",
      "[296]\ttrain-rmse:0.071815\teval-rmse:0.127874\n",
      "[297]\ttrain-rmse:0.071261\teval-rmse:0.127615\n",
      "[298]\ttrain-rmse:0.07085\teval-rmse:0.12741\n",
      "[299]\ttrain-rmse:0.070339\teval-rmse:0.127182\n",
      "[300]\ttrain-rmse:0.069949\teval-rmse:0.126991\n",
      "[301]\ttrain-rmse:0.069548\teval-rmse:0.126784\n",
      "[302]\ttrain-rmse:0.069112\teval-rmse:0.126546\n",
      "[303]\ttrain-rmse:0.068784\teval-rmse:0.126382\n",
      "[304]\ttrain-rmse:0.06839\teval-rmse:0.126186\n",
      "[305]\ttrain-rmse:0.067903\teval-rmse:0.125993\n",
      "[306]\ttrain-rmse:0.067612\teval-rmse:0.125858\n",
      "[307]\ttrain-rmse:0.067318\teval-rmse:0.125713\n",
      "[308]\ttrain-rmse:0.066917\teval-rmse:0.125497\n",
      "[309]\ttrain-rmse:0.06662\teval-rmse:0.125386\n",
      "[310]\ttrain-rmse:0.066346\teval-rmse:0.125241\n",
      "[311]\ttrain-rmse:0.066006\teval-rmse:0.125062\n",
      "[312]\ttrain-rmse:0.065747\teval-rmse:0.124907\n",
      "[313]\ttrain-rmse:0.06538\teval-rmse:0.124725\n",
      "[314]\ttrain-rmse:0.065146\teval-rmse:0.124626\n",
      "[315]\ttrain-rmse:0.064896\teval-rmse:0.124493\n",
      "[316]\ttrain-rmse:0.064562\teval-rmse:0.124319\n",
      "[317]\ttrain-rmse:0.064347\teval-rmse:0.124239\n",
      "[318]\ttrain-rmse:0.064018\teval-rmse:0.124159\n",
      "[319]\ttrain-rmse:0.063796\teval-rmse:0.124051\n",
      "[320]\ttrain-rmse:0.06347\teval-rmse:0.123915\n",
      "[321]\ttrain-rmse:0.063167\teval-rmse:0.123762\n",
      "[322]\ttrain-rmse:0.062898\teval-rmse:0.123679\n",
      "[323]\ttrain-rmse:0.062604\teval-rmse:0.123602\n",
      "[324]\ttrain-rmse:0.062416\teval-rmse:0.123493\n",
      "[325]\ttrain-rmse:0.062126\teval-rmse:0.123441\n",
      "[326]\ttrain-rmse:0.061879\teval-rmse:0.123337\n",
      "[327]\ttrain-rmse:0.061635\teval-rmse:0.123267\n",
      "[328]\ttrain-rmse:0.061381\teval-rmse:0.123128\n",
      "[329]\ttrain-rmse:0.060993\teval-rmse:0.123005\n",
      "[330]\ttrain-rmse:0.060821\teval-rmse:0.122922\n",
      "[331]\ttrain-rmse:0.060555\teval-rmse:0.122816\n",
      "[332]\ttrain-rmse:0.060332\teval-rmse:0.122753\n",
      "[333]\ttrain-rmse:0.0601\teval-rmse:0.122626\n",
      "[334]\ttrain-rmse:0.059936\teval-rmse:0.122567\n",
      "[335]\ttrain-rmse:0.059682\teval-rmse:0.12247\n",
      "[336]\ttrain-rmse:0.059476\teval-rmse:0.12242\n",
      "[337]\ttrain-rmse:0.059113\teval-rmse:0.12236\n",
      "[338]\ttrain-rmse:0.058903\teval-rmse:0.122238\n",
      "[339]\ttrain-rmse:0.058735\teval-rmse:0.122171\n",
      "[340]\ttrain-rmse:0.058403\teval-rmse:0.122129\n",
      "[341]\ttrain-rmse:0.058173\teval-rmse:0.122047\n",
      "[342]\ttrain-rmse:0.058039\teval-rmse:0.121983\n",
      "[343]\ttrain-rmse:0.057827\teval-rmse:0.121886\n",
      "[344]\ttrain-rmse:0.057693\teval-rmse:0.121828\n",
      "[345]\ttrain-rmse:0.057516\teval-rmse:0.121764\n",
      "[346]\ttrain-rmse:0.057195\teval-rmse:0.121723\n",
      "[347]\ttrain-rmse:0.056998\teval-rmse:0.121694\n",
      "[348]\ttrain-rmse:0.056866\teval-rmse:0.121613\n",
      "[349]\ttrain-rmse:0.056557\teval-rmse:0.121561\n",
      "[350]\ttrain-rmse:0.056372\teval-rmse:0.121493\n",
      "[351]\ttrain-rmse:0.056165\teval-rmse:0.121418\n",
      "[352]\ttrain-rmse:0.056056\teval-rmse:0.121358\n",
      "[353]\ttrain-rmse:0.055878\teval-rmse:0.121314\n",
      "[354]\ttrain-rmse:0.055634\teval-rmse:0.121303\n",
      "[355]\ttrain-rmse:0.055485\teval-rmse:0.121233\n",
      "[356]\ttrain-rmse:0.055311\teval-rmse:0.121208\n",
      "[357]\ttrain-rmse:0.055059\teval-rmse:0.121134\n",
      "[358]\ttrain-rmse:0.054951\teval-rmse:0.121104\n",
      "[359]\ttrain-rmse:0.05476\teval-rmse:0.121038\n",
      "[360]\ttrain-rmse:0.054652\teval-rmse:0.120985\n",
      "[361]\ttrain-rmse:0.054414\teval-rmse:0.120919\n",
      "[362]\ttrain-rmse:0.054265\teval-rmse:0.120865\n",
      "[363]\ttrain-rmse:0.054098\teval-rmse:0.120827\n",
      "[364]\ttrain-rmse:0.053954\teval-rmse:0.120773\n",
      "[365]\ttrain-rmse:0.053728\teval-rmse:0.120708\n",
      "[366]\ttrain-rmse:0.05364\teval-rmse:0.120662\n",
      "[367]\ttrain-rmse:0.053532\teval-rmse:0.120591\n",
      "[368]\ttrain-rmse:0.053418\teval-rmse:0.120546\n",
      "[369]\ttrain-rmse:0.05331\teval-rmse:0.120499\n",
      "[370]\ttrain-rmse:0.053191\teval-rmse:0.120448\n",
      "[371]\ttrain-rmse:0.052999\teval-rmse:0.120391\n",
      "[372]\ttrain-rmse:0.052889\teval-rmse:0.120345\n",
      "[373]\ttrain-rmse:0.052676\teval-rmse:0.120299\n",
      "[374]\ttrain-rmse:0.052472\teval-rmse:0.120246\n",
      "[375]\ttrain-rmse:0.052378\teval-rmse:0.120212\n",
      "[376]\ttrain-rmse:0.052247\teval-rmse:0.120143\n",
      "[377]\ttrain-rmse:0.052132\teval-rmse:0.120112\n",
      "[378]\ttrain-rmse:0.05193\teval-rmse:0.120045\n",
      "[379]\ttrain-rmse:0.051769\teval-rmse:0.119994\n",
      "[380]\ttrain-rmse:0.051629\teval-rmse:0.11998\n",
      "[381]\ttrain-rmse:0.051477\teval-rmse:0.119947\n",
      "[382]\ttrain-rmse:0.051339\teval-rmse:0.119895\n",
      "[383]\ttrain-rmse:0.051215\teval-rmse:0.119879\n",
      "[384]\ttrain-rmse:0.051115\teval-rmse:0.119838\n",
      "[385]\ttrain-rmse:0.05097\teval-rmse:0.119786\n",
      "[386]\ttrain-rmse:0.050823\teval-rmse:0.119728\n",
      "[387]\ttrain-rmse:0.0507\teval-rmse:0.119684\n",
      "[388]\ttrain-rmse:0.050604\teval-rmse:0.119644\n",
      "[389]\ttrain-rmse:0.050526\teval-rmse:0.119609\n",
      "[390]\ttrain-rmse:0.050419\teval-rmse:0.119563\n",
      "[391]\ttrain-rmse:0.050297\teval-rmse:0.119487\n",
      "[392]\ttrain-rmse:0.050122\teval-rmse:0.119438\n",
      "[393]\ttrain-rmse:0.049961\teval-rmse:0.119399\n",
      "[394]\ttrain-rmse:0.049819\teval-rmse:0.119357\n",
      "[395]\ttrain-rmse:0.049682\teval-rmse:0.119313\n",
      "[396]\ttrain-rmse:0.049568\teval-rmse:0.119297\n",
      "[397]\ttrain-rmse:0.049395\teval-rmse:0.119278\n",
      "[398]\ttrain-rmse:0.04928\teval-rmse:0.119264\n",
      "[399]\ttrain-rmse:0.04921\teval-rmse:0.119236\n",
      "[400]\ttrain-rmse:0.049059\teval-rmse:0.119198\n",
      "[401]\ttrain-rmse:0.048963\teval-rmse:0.119169\n",
      "[402]\ttrain-rmse:0.048859\teval-rmse:0.119129\n",
      "[403]\ttrain-rmse:0.048753\teval-rmse:0.119075\n",
      "[404]\ttrain-rmse:0.048607\teval-rmse:0.119045\n",
      "[405]\ttrain-rmse:0.048536\teval-rmse:0.119011\n",
      "[406]\ttrain-rmse:0.048402\teval-rmse:0.118979\n",
      "[407]\ttrain-rmse:0.048254\teval-rmse:0.118953\n",
      "[408]\ttrain-rmse:0.048084\teval-rmse:0.118897\n",
      "[409]\ttrain-rmse:0.047957\teval-rmse:0.118876\n",
      "[410]\ttrain-rmse:0.047872\teval-rmse:0.118852\n",
      "[411]\ttrain-rmse:0.047767\teval-rmse:0.118845\n",
      "[412]\ttrain-rmse:0.04768\teval-rmse:0.11883\n",
      "[413]\ttrain-rmse:0.047606\teval-rmse:0.118804\n",
      "[414]\ttrain-rmse:0.047508\teval-rmse:0.118796\n",
      "[415]\ttrain-rmse:0.047444\teval-rmse:0.11877\n",
      "[416]\ttrain-rmse:0.04732\teval-rmse:0.118747\n",
      "[417]\ttrain-rmse:0.047214\teval-rmse:0.11874\n",
      "[418]\ttrain-rmse:0.047116\teval-rmse:0.118697\n",
      "[419]\ttrain-rmse:0.046992\teval-rmse:0.118674\n",
      "[420]\ttrain-rmse:0.046873\teval-rmse:0.118648\n",
      "[421]\ttrain-rmse:0.046812\teval-rmse:0.118624\n",
      "[422]\ttrain-rmse:0.046714\teval-rmse:0.118613\n",
      "[423]\ttrain-rmse:0.046622\teval-rmse:0.118603\n",
      "[424]\ttrain-rmse:0.046501\teval-rmse:0.118587\n",
      "[425]\ttrain-rmse:0.046387\teval-rmse:0.118567\n",
      "[426]\ttrain-rmse:0.046332\teval-rmse:0.118547\n",
      "[427]\ttrain-rmse:0.046232\teval-rmse:0.118519\n",
      "[428]\ttrain-rmse:0.046062\teval-rmse:0.118513\n",
      "[429]\ttrain-rmse:0.045957\teval-rmse:0.118509\n",
      "[430]\ttrain-rmse:0.045872\teval-rmse:0.118486\n",
      "[431]\ttrain-rmse:0.045755\teval-rmse:0.118469\n",
      "[432]\ttrain-rmse:0.045645\teval-rmse:0.118451\n",
      "[433]\ttrain-rmse:0.04555\teval-rmse:0.118436\n",
      "[434]\ttrain-rmse:0.045435\teval-rmse:0.118387\n",
      "[435]\ttrain-rmse:0.045325\teval-rmse:0.118367\n",
      "[436]\ttrain-rmse:0.045221\teval-rmse:0.118362\n",
      "[437]\ttrain-rmse:0.045125\teval-rmse:0.118352\n",
      "[438]\ttrain-rmse:0.045007\teval-rmse:0.118321\n",
      "[439]\ttrain-rmse:0.044902\teval-rmse:0.118294\n",
      "[440]\ttrain-rmse:0.044802\teval-rmse:0.118291\n",
      "[441]\ttrain-rmse:0.044683\teval-rmse:0.118288\n",
      "[442]\ttrain-rmse:0.044622\teval-rmse:0.118254\n",
      "[443]\ttrain-rmse:0.044519\teval-rmse:0.118242\n",
      "[444]\ttrain-rmse:0.04446\teval-rmse:0.11821\n",
      "[445]\ttrain-rmse:0.044363\teval-rmse:0.118205\n",
      "[446]\ttrain-rmse:0.044231\teval-rmse:0.118181\n",
      "[447]\ttrain-rmse:0.044115\teval-rmse:0.118178\n",
      "[448]\ttrain-rmse:0.044013\teval-rmse:0.11814\n",
      "[449]\ttrain-rmse:0.043856\teval-rmse:0.118092\n",
      "[450]\ttrain-rmse:0.043749\teval-rmse:0.118088\n",
      "[451]\ttrain-rmse:0.043639\teval-rmse:0.118091\n",
      "[452]\ttrain-rmse:0.043576\teval-rmse:0.118053\n",
      "[453]\ttrain-rmse:0.043481\teval-rmse:0.118047\n",
      "[454]\ttrain-rmse:0.043382\teval-rmse:0.118039\n",
      "[455]\ttrain-rmse:0.043238\teval-rmse:0.117987\n",
      "[456]\ttrain-rmse:0.043169\teval-rmse:0.117988\n",
      "[457]\ttrain-rmse:0.043108\teval-rmse:0.11795\n",
      "[458]\ttrain-rmse:0.043012\teval-rmse:0.117919\n",
      "[459]\ttrain-rmse:0.042914\teval-rmse:0.117899\n",
      "[460]\ttrain-rmse:0.042781\teval-rmse:0.117888\n",
      "[461]\ttrain-rmse:0.042685\teval-rmse:0.117857\n",
      "[462]\ttrain-rmse:0.042598\teval-rmse:0.117849\n",
      "[463]\ttrain-rmse:0.042526\teval-rmse:0.117827\n",
      "[464]\ttrain-rmse:0.042437\teval-rmse:0.117802\n",
      "[465]\ttrain-rmse:0.042309\teval-rmse:0.117793\n",
      "[466]\ttrain-rmse:0.042251\teval-rmse:0.117796\n",
      "[467]\ttrain-rmse:0.042183\teval-rmse:0.117794\n",
      "[468]\ttrain-rmse:0.042044\teval-rmse:0.11779\n",
      "[469]\ttrain-rmse:0.04196\teval-rmse:0.117779\n",
      "[470]\ttrain-rmse:0.041865\teval-rmse:0.117769\n",
      "[471]\ttrain-rmse:0.04174\teval-rmse:0.11775\n",
      "[472]\ttrain-rmse:0.041651\teval-rmse:0.117726\n",
      "[473]\ttrain-rmse:0.041523\teval-rmse:0.117691\n",
      "[474]\ttrain-rmse:0.041445\teval-rmse:0.117689\n",
      "[475]\ttrain-rmse:0.041325\teval-rmse:0.117671\n",
      "[476]\ttrain-rmse:0.041198\teval-rmse:0.117628\n",
      "[477]\ttrain-rmse:0.041106\teval-rmse:0.117627\n",
      "[478]\ttrain-rmse:0.041041\teval-rmse:0.117608\n",
      "[479]\ttrain-rmse:0.040917\teval-rmse:0.117589\n",
      "[480]\ttrain-rmse:0.040796\teval-rmse:0.117572\n",
      "[481]\ttrain-rmse:0.040731\teval-rmse:0.117559\n",
      "[482]\ttrain-rmse:0.040643\teval-rmse:0.117555\n",
      "[483]\ttrain-rmse:0.040521\teval-rmse:0.117508\n",
      "[484]\ttrain-rmse:0.040442\teval-rmse:0.117498\n",
      "[485]\ttrain-rmse:0.040377\teval-rmse:0.117467\n",
      "[486]\ttrain-rmse:0.040264\teval-rmse:0.117444\n",
      "[487]\ttrain-rmse:0.040134\teval-rmse:0.117426\n",
      "[488]\ttrain-rmse:0.040028\teval-rmse:0.11743\n",
      "[489]\ttrain-rmse:0.039973\teval-rmse:0.117437\n",
      "[490]\ttrain-rmse:0.039885\teval-rmse:0.117429\n",
      "[491]\ttrain-rmse:0.039833\teval-rmse:0.117418\n",
      "[492]\ttrain-rmse:0.039752\teval-rmse:0.117395\n",
      "[493]\ttrain-rmse:0.039667\teval-rmse:0.117392\n",
      "[494]\ttrain-rmse:0.03958\teval-rmse:0.117389\n",
      "[495]\ttrain-rmse:0.039453\teval-rmse:0.117376\n",
      "[496]\ttrain-rmse:0.039423\teval-rmse:0.117367\n",
      "[497]\ttrain-rmse:0.039359\teval-rmse:0.117357\n",
      "[498]\ttrain-rmse:0.03926\teval-rmse:0.117364\n",
      "[499]\ttrain-rmse:0.039206\teval-rmse:0.11736\n",
      "[500]\ttrain-rmse:0.039172\teval-rmse:0.117339\n",
      "[501]\ttrain-rmse:0.03907\teval-rmse:0.117326\n",
      "[502]\ttrain-rmse:0.038972\teval-rmse:0.117317\n",
      "[503]\ttrain-rmse:0.038891\teval-rmse:0.117312\n",
      "[504]\ttrain-rmse:0.038769\teval-rmse:0.117295\n",
      "[505]\ttrain-rmse:0.038691\teval-rmse:0.11728\n",
      "[506]\ttrain-rmse:0.03861\teval-rmse:0.117274\n",
      "[507]\ttrain-rmse:0.038517\teval-rmse:0.117228\n",
      "[508]\ttrain-rmse:0.038396\teval-rmse:0.117209\n",
      "[509]\ttrain-rmse:0.038325\teval-rmse:0.117196\n",
      "[510]\ttrain-rmse:0.038282\teval-rmse:0.117204\n",
      "[511]\ttrain-rmse:0.038194\teval-rmse:0.117173\n",
      "[512]\ttrain-rmse:0.03814\teval-rmse:0.117155\n",
      "[513]\ttrain-rmse:0.038026\teval-rmse:0.117159\n",
      "[514]\ttrain-rmse:0.037954\teval-rmse:0.117156\n",
      "[515]\ttrain-rmse:0.037893\teval-rmse:0.117145\n",
      "[516]\ttrain-rmse:0.037849\teval-rmse:0.117115\n",
      "[517]\ttrain-rmse:0.037782\teval-rmse:0.117104\n",
      "[518]\ttrain-rmse:0.037688\teval-rmse:0.117082\n",
      "[519]\ttrain-rmse:0.037657\teval-rmse:0.117086\n",
      "[520]\ttrain-rmse:0.037614\teval-rmse:0.117059\n",
      "[521]\ttrain-rmse:0.037505\teval-rmse:0.117049\n",
      "[522]\ttrain-rmse:0.037425\teval-rmse:0.117017\n",
      "[523]\ttrain-rmse:0.037301\teval-rmse:0.117012\n",
      "[524]\ttrain-rmse:0.037239\teval-rmse:0.116996\n",
      "[525]\ttrain-rmse:0.037171\teval-rmse:0.116979\n",
      "[526]\ttrain-rmse:0.037098\teval-rmse:0.11696\n",
      "[527]\ttrain-rmse:0.037034\teval-rmse:0.11693\n",
      "[528]\ttrain-rmse:0.036956\teval-rmse:0.116926\n",
      "[529]\ttrain-rmse:0.036854\teval-rmse:0.116937\n",
      "[530]\ttrain-rmse:0.036806\teval-rmse:0.116942\n",
      "[531]\ttrain-rmse:0.036692\teval-rmse:0.116916\n",
      "[532]\ttrain-rmse:0.036669\teval-rmse:0.116909\n",
      "[533]\ttrain-rmse:0.036592\teval-rmse:0.116898\n",
      "[534]\ttrain-rmse:0.036521\teval-rmse:0.116882\n",
      "[535]\ttrain-rmse:0.036486\teval-rmse:0.116874\n",
      "[536]\ttrain-rmse:0.036385\teval-rmse:0.116862\n",
      "[537]\ttrain-rmse:0.036364\teval-rmse:0.116856\n",
      "[538]\ttrain-rmse:0.036282\teval-rmse:0.116842\n",
      "[539]\ttrain-rmse:0.036175\teval-rmse:0.116815\n",
      "[540]\ttrain-rmse:0.036039\teval-rmse:0.116815\n",
      "[541]\ttrain-rmse:0.035892\teval-rmse:0.116798\n",
      "[542]\ttrain-rmse:0.035833\teval-rmse:0.11679\n",
      "[543]\ttrain-rmse:0.035813\teval-rmse:0.116786\n",
      "[544]\ttrain-rmse:0.035756\teval-rmse:0.116775\n",
      "[545]\ttrain-rmse:0.035622\teval-rmse:0.116776\n",
      "[546]\ttrain-rmse:0.035576\teval-rmse:0.11677\n",
      "[547]\ttrain-rmse:0.035477\teval-rmse:0.116748\n",
      "[548]\ttrain-rmse:0.035373\teval-rmse:0.116714\n",
      "[549]\ttrain-rmse:0.03532\teval-rmse:0.116703\n",
      "[550]\ttrain-rmse:0.035243\teval-rmse:0.116689\n",
      "[551]\ttrain-rmse:0.035211\teval-rmse:0.116672\n",
      "[552]\ttrain-rmse:0.03508\teval-rmse:0.116674\n",
      "[553]\ttrain-rmse:0.034975\teval-rmse:0.116679\n",
      "[554]\ttrain-rmse:0.034884\teval-rmse:0.116667\n",
      "[555]\ttrain-rmse:0.034745\teval-rmse:0.116659\n",
      "[556]\ttrain-rmse:0.034703\teval-rmse:0.116645\n",
      "[557]\ttrain-rmse:0.034648\teval-rmse:0.11664\n",
      "[558]\ttrain-rmse:0.034515\teval-rmse:0.116625\n",
      "[559]\ttrain-rmse:0.034481\teval-rmse:0.116621\n",
      "[560]\ttrain-rmse:0.034417\teval-rmse:0.116591\n",
      "[561]\ttrain-rmse:0.03427\teval-rmse:0.116606\n",
      "[562]\ttrain-rmse:0.034247\teval-rmse:0.116605\n",
      "[563]\ttrain-rmse:0.034179\teval-rmse:0.11657\n",
      "[564]\ttrain-rmse:0.034133\teval-rmse:0.116547\n",
      "[565]\ttrain-rmse:0.034114\teval-rmse:0.116544\n",
      "[566]\ttrain-rmse:0.034065\teval-rmse:0.116545\n",
      "[567]\ttrain-rmse:0.034\teval-rmse:0.116543\n",
      "[568]\ttrain-rmse:0.033982\teval-rmse:0.116539\n",
      "[569]\ttrain-rmse:0.033877\teval-rmse:0.116537\n",
      "[570]\ttrain-rmse:0.033775\teval-rmse:0.116524\n",
      "[571]\ttrain-rmse:0.033715\teval-rmse:0.116513\n",
      "[572]\ttrain-rmse:0.033683\teval-rmse:0.116509\n",
      "[573]\ttrain-rmse:0.033554\teval-rmse:0.116501\n",
      "[574]\ttrain-rmse:0.03352\teval-rmse:0.116492\n",
      "[575]\ttrain-rmse:0.033408\teval-rmse:0.116487\n",
      "[576]\ttrain-rmse:0.033378\teval-rmse:0.116482\n",
      "[577]\ttrain-rmse:0.03329\teval-rmse:0.116479\n",
      "[578]\ttrain-rmse:0.033234\teval-rmse:0.116452\n",
      "[579]\ttrain-rmse:0.033114\teval-rmse:0.116444\n",
      "[580]\ttrain-rmse:0.033076\teval-rmse:0.116434\n",
      "[581]\ttrain-rmse:0.033004\teval-rmse:0.116436\n",
      "[582]\ttrain-rmse:0.032987\teval-rmse:0.116433\n",
      "[583]\ttrain-rmse:0.032938\teval-rmse:0.116422\n",
      "[584]\ttrain-rmse:0.032821\teval-rmse:0.116413\n",
      "[585]\ttrain-rmse:0.032789\teval-rmse:0.116407\n",
      "[586]\ttrain-rmse:0.032666\teval-rmse:0.116402\n",
      "[587]\ttrain-rmse:0.032581\teval-rmse:0.116392\n",
      "[588]\ttrain-rmse:0.032557\teval-rmse:0.1164\n",
      "[589]\ttrain-rmse:0.032464\teval-rmse:0.116402\n",
      "[590]\ttrain-rmse:0.032446\teval-rmse:0.116399\n",
      "[591]\ttrain-rmse:0.032402\teval-rmse:0.116401\n",
      "[592]\ttrain-rmse:0.032367\teval-rmse:0.116395\n",
      "[593]\ttrain-rmse:0.032286\teval-rmse:0.116395\n",
      "[594]\ttrain-rmse:0.032219\teval-rmse:0.11638\n",
      "[595]\ttrain-rmse:0.032201\teval-rmse:0.116381\n",
      "[596]\ttrain-rmse:0.032123\teval-rmse:0.116375\n",
      "[597]\ttrain-rmse:0.032077\teval-rmse:0.116364\n",
      "[598]\ttrain-rmse:0.032013\teval-rmse:0.116365\n",
      "[599]\ttrain-rmse:0.031987\teval-rmse:0.116361\n",
      "[600]\ttrain-rmse:0.031974\teval-rmse:0.116353\n",
      "[601]\ttrain-rmse:0.031905\teval-rmse:0.116344\n",
      "[602]\ttrain-rmse:0.031821\teval-rmse:0.116358\n",
      "[603]\ttrain-rmse:0.031772\teval-rmse:0.116343\n",
      "[604]\ttrain-rmse:0.031669\teval-rmse:0.116336\n",
      "[605]\ttrain-rmse:0.031564\teval-rmse:0.11634\n",
      "[606]\ttrain-rmse:0.031519\teval-rmse:0.116327\n",
      "[607]\ttrain-rmse:0.031438\teval-rmse:0.116323\n",
      "[608]\ttrain-rmse:0.031371\teval-rmse:0.116312\n",
      "[609]\ttrain-rmse:0.031358\teval-rmse:0.116304\n",
      "[610]\ttrain-rmse:0.031323\teval-rmse:0.116295\n",
      "[611]\ttrain-rmse:0.031246\teval-rmse:0.116305\n",
      "[612]\ttrain-rmse:0.03117\teval-rmse:0.116291\n",
      "[613]\ttrain-rmse:0.031128\teval-rmse:0.116283\n",
      "[614]\ttrain-rmse:0.031071\teval-rmse:0.116279\n",
      "[615]\ttrain-rmse:0.030941\teval-rmse:0.116281\n",
      "[616]\ttrain-rmse:0.030889\teval-rmse:0.116289\n",
      "[617]\ttrain-rmse:0.030763\teval-rmse:0.116286\n",
      "[618]\ttrain-rmse:0.030703\teval-rmse:0.116282\n",
      "[619]\ttrain-rmse:0.030654\teval-rmse:0.116263\n",
      "[620]\ttrain-rmse:0.030616\teval-rmse:0.116246\n",
      "[621]\ttrain-rmse:0.030551\teval-rmse:0.116239\n",
      "[622]\ttrain-rmse:0.030511\teval-rmse:0.116223\n",
      "[623]\ttrain-rmse:0.030488\teval-rmse:0.116224\n",
      "[624]\ttrain-rmse:0.030454\teval-rmse:0.116215\n",
      "[625]\ttrain-rmse:0.030439\teval-rmse:0.116207\n",
      "[626]\ttrain-rmse:0.030377\teval-rmse:0.116197\n",
      "[627]\ttrain-rmse:0.030282\teval-rmse:0.116193\n",
      "[628]\ttrain-rmse:0.030244\teval-rmse:0.116184\n",
      "[629]\ttrain-rmse:0.030186\teval-rmse:0.116185\n",
      "[630]\ttrain-rmse:0.030126\teval-rmse:0.116181\n",
      "[631]\ttrain-rmse:0.030069\teval-rmse:0.116173\n",
      "[632]\ttrain-rmse:0.030016\teval-rmse:0.116161\n",
      "[633]\ttrain-rmse:0.029961\teval-rmse:0.116161\n",
      "[634]\ttrain-rmse:0.029869\teval-rmse:0.116153\n",
      "[635]\ttrain-rmse:0.029818\teval-rmse:0.116153\n",
      "[636]\ttrain-rmse:0.029753\teval-rmse:0.116141\n",
      "[637]\ttrain-rmse:0.02973\teval-rmse:0.116136\n",
      "[638]\ttrain-rmse:0.029677\teval-rmse:0.116129\n",
      "[639]\ttrain-rmse:0.029606\teval-rmse:0.116122\n",
      "[640]\ttrain-rmse:0.02955\teval-rmse:0.116116\n",
      "[641]\ttrain-rmse:0.029531\teval-rmse:0.116117\n",
      "[642]\ttrain-rmse:0.029474\teval-rmse:0.116119\n",
      "[643]\ttrain-rmse:0.029424\teval-rmse:0.116121\n",
      "[644]\ttrain-rmse:0.029368\teval-rmse:0.11611\n",
      "[645]\ttrain-rmse:0.029349\teval-rmse:0.116111\n",
      "[646]\ttrain-rmse:0.02929\teval-rmse:0.116116\n",
      "[647]\ttrain-rmse:0.029214\teval-rmse:0.116105\n",
      "[648]\ttrain-rmse:0.029161\teval-rmse:0.11609\n",
      "[649]\ttrain-rmse:0.029107\teval-rmse:0.116087\n",
      "[650]\ttrain-rmse:0.029062\teval-rmse:0.116093\n",
      "[651]\ttrain-rmse:0.029005\teval-rmse:0.116096\n",
      "[652]\ttrain-rmse:0.02899\teval-rmse:0.116094\n",
      "[653]\ttrain-rmse:0.028934\teval-rmse:0.116099\n",
      "[654]\ttrain-rmse:0.028842\teval-rmse:0.116082\n",
      "[655]\ttrain-rmse:0.028792\teval-rmse:0.116085\n",
      "[656]\ttrain-rmse:0.028721\teval-rmse:0.116091\n",
      "[657]\ttrain-rmse:0.028664\teval-rmse:0.116078\n",
      "[658]\ttrain-rmse:0.028607\teval-rmse:0.116065\n",
      "[659]\ttrain-rmse:0.028589\teval-rmse:0.116068\n",
      "[660]\ttrain-rmse:0.028546\teval-rmse:0.116053\n",
      "[661]\ttrain-rmse:0.028496\teval-rmse:0.116047\n",
      "[662]\ttrain-rmse:0.028428\teval-rmse:0.116059\n",
      "[663]\ttrain-rmse:0.028352\teval-rmse:0.116057\n",
      "[664]\ttrain-rmse:0.028332\teval-rmse:0.116054\n",
      "[665]\ttrain-rmse:0.028298\teval-rmse:0.116041\n",
      "[666]\ttrain-rmse:0.028243\teval-rmse:0.116024\n",
      "[667]\ttrain-rmse:0.028189\teval-rmse:0.116027\n",
      "[668]\ttrain-rmse:0.028167\teval-rmse:0.116024\n",
      "[669]\ttrain-rmse:0.028117\teval-rmse:0.116018\n",
      "[670]\ttrain-rmse:0.028064\teval-rmse:0.116012\n",
      "[671]\ttrain-rmse:0.028054\teval-rmse:0.116009\n",
      "[672]\ttrain-rmse:0.027982\teval-rmse:0.11601\n",
      "[673]\ttrain-rmse:0.027935\teval-rmse:0.115996\n",
      "[674]\ttrain-rmse:0.027878\teval-rmse:0.115986\n",
      "[675]\ttrain-rmse:0.027861\teval-rmse:0.115987\n",
      "[676]\ttrain-rmse:0.027832\teval-rmse:0.115983\n",
      "[677]\ttrain-rmse:0.027787\teval-rmse:0.115974\n",
      "[678]\ttrain-rmse:0.027712\teval-rmse:0.11596\n",
      "[679]\ttrain-rmse:0.027657\teval-rmse:0.115946\n",
      "[680]\ttrain-rmse:0.02759\teval-rmse:0.115948\n",
      "[681]\ttrain-rmse:0.027535\teval-rmse:0.115943\n",
      "[682]\ttrain-rmse:0.027494\teval-rmse:0.115948\n",
      "[683]\ttrain-rmse:0.027476\teval-rmse:0.11595\n",
      "[684]\ttrain-rmse:0.027404\teval-rmse:0.115932\n",
      "[685]\ttrain-rmse:0.027365\teval-rmse:0.11593\n",
      "[686]\ttrain-rmse:0.027337\teval-rmse:0.115926\n",
      "[687]\ttrain-rmse:0.027285\teval-rmse:0.115913\n",
      "[688]\ttrain-rmse:0.02722\teval-rmse:0.115923\n",
      "[689]\ttrain-rmse:0.027153\teval-rmse:0.11592\n",
      "[690]\ttrain-rmse:0.02711\teval-rmse:0.115913\n",
      "[691]\ttrain-rmse:0.027058\teval-rmse:0.115907\n",
      "[692]\ttrain-rmse:0.027036\teval-rmse:0.115907\n",
      "[693]\ttrain-rmse:0.027008\teval-rmse:0.115903\n",
      "[694]\ttrain-rmse:0.026941\teval-rmse:0.115909\n",
      "[695]\ttrain-rmse:0.026868\teval-rmse:0.115913\n",
      "[696]\ttrain-rmse:0.026849\teval-rmse:0.115911\n",
      "[697]\ttrain-rmse:0.026785\teval-rmse:0.115921\n",
      "[698]\ttrain-rmse:0.026713\teval-rmse:0.115917\n",
      "[699]\ttrain-rmse:0.026662\teval-rmse:0.115907\n",
      "[700]\ttrain-rmse:0.026597\teval-rmse:0.115908\n",
      "[701]\ttrain-rmse:0.026536\teval-rmse:0.115902\n",
      "[702]\ttrain-rmse:0.026492\teval-rmse:0.115903\n",
      "[703]\ttrain-rmse:0.026475\teval-rmse:0.115902\n",
      "[704]\ttrain-rmse:0.026425\teval-rmse:0.115886\n",
      "[705]\ttrain-rmse:0.026362\teval-rmse:0.115894\n",
      "[706]\ttrain-rmse:0.026335\teval-rmse:0.115899\n",
      "[707]\ttrain-rmse:0.02631\teval-rmse:0.115895\n",
      "[708]\ttrain-rmse:0.026262\teval-rmse:0.115891\n",
      "[709]\ttrain-rmse:0.026213\teval-rmse:0.115901\n",
      "[710]\ttrain-rmse:0.026168\teval-rmse:0.115888\n",
      "[711]\ttrain-rmse:0.026147\teval-rmse:0.115888\n",
      "[712]\ttrain-rmse:0.026087\teval-rmse:0.115877\n",
      "[713]\ttrain-rmse:0.026031\teval-rmse:0.115876\n",
      "[714]\ttrain-rmse:0.025994\teval-rmse:0.115872\n",
      "[715]\ttrain-rmse:0.025925\teval-rmse:0.115853\n",
      "[716]\ttrain-rmse:0.025868\teval-rmse:0.115846\n",
      "[717]\ttrain-rmse:0.025792\teval-rmse:0.115835\n",
      "[718]\ttrain-rmse:0.025768\teval-rmse:0.11583\n",
      "[719]\ttrain-rmse:0.025724\teval-rmse:0.115822\n",
      "[720]\ttrain-rmse:0.025654\teval-rmse:0.115812\n",
      "[721]\ttrain-rmse:0.02561\teval-rmse:0.11581\n",
      "[722]\ttrain-rmse:0.025572\teval-rmse:0.115816\n",
      "[723]\ttrain-rmse:0.02556\teval-rmse:0.115814\n",
      "[724]\ttrain-rmse:0.025551\teval-rmse:0.115816\n",
      "[725]\ttrain-rmse:0.025516\teval-rmse:0.115819\n",
      "[726]\ttrain-rmse:0.025491\teval-rmse:0.115819\n",
      "[727]\ttrain-rmse:0.025418\teval-rmse:0.115828\n",
      "[728]\ttrain-rmse:0.025355\teval-rmse:0.11584\n",
      "[729]\ttrain-rmse:0.025286\teval-rmse:0.115846\n",
      "[730]\ttrain-rmse:0.025246\teval-rmse:0.115847\n",
      "[731]\ttrain-rmse:0.025222\teval-rmse:0.11584\n",
      "[732]\ttrain-rmse:0.025196\teval-rmse:0.115833\n",
      "[733]\ttrain-rmse:0.025151\teval-rmse:0.11584\n",
      "[734]\ttrain-rmse:0.025133\teval-rmse:0.115838\n",
      "[735]\ttrain-rmse:0.025064\teval-rmse:0.115838\n",
      "[736]\ttrain-rmse:0.025042\teval-rmse:0.115833\n",
      "[737]\ttrain-rmse:0.025008\teval-rmse:0.115824\n",
      "[738]\ttrain-rmse:0.024973\teval-rmse:0.115826\n",
      "[739]\ttrain-rmse:0.024917\teval-rmse:0.115821\n",
      "[740]\ttrain-rmse:0.024901\teval-rmse:0.115814\n",
      "[741]\ttrain-rmse:0.024868\teval-rmse:0.115818\n",
      "[742]\ttrain-rmse:0.024844\teval-rmse:0.115812\n",
      "[743]\ttrain-rmse:0.024774\teval-rmse:0.11582\n",
      "[744]\ttrain-rmse:0.024703\teval-rmse:0.115812\n",
      "[745]\ttrain-rmse:0.024694\teval-rmse:0.115806\n",
      "[746]\ttrain-rmse:0.024624\teval-rmse:0.11582\n",
      "[747]\ttrain-rmse:0.024556\teval-rmse:0.115813\n",
      "[748]\ttrain-rmse:0.024519\teval-rmse:0.115815\n",
      "[749]\ttrain-rmse:0.024504\teval-rmse:0.115815\n",
      "[750]\ttrain-rmse:0.024456\teval-rmse:0.115809\n",
      "[751]\ttrain-rmse:0.024422\teval-rmse:0.1158\n",
      "[752]\ttrain-rmse:0.024389\teval-rmse:0.1158\n",
      "[753]\ttrain-rmse:0.024381\teval-rmse:0.115803\n",
      "[754]\ttrain-rmse:0.024339\teval-rmse:0.115804\n",
      "[755]\ttrain-rmse:0.024271\teval-rmse:0.115817\n",
      "[756]\ttrain-rmse:0.024206\teval-rmse:0.115819\n",
      "[757]\ttrain-rmse:0.024167\teval-rmse:0.115824\n",
      "[758]\ttrain-rmse:0.024144\teval-rmse:0.115822\n",
      "[759]\ttrain-rmse:0.024111\teval-rmse:0.115819\n",
      "[760]\ttrain-rmse:0.024104\teval-rmse:0.115823\n",
      "[761]\ttrain-rmse:0.024081\teval-rmse:0.11582\n",
      "[762]\ttrain-rmse:0.02404\teval-rmse:0.115819\n",
      "[763]\ttrain-rmse:0.024006\teval-rmse:0.115824\n",
      "[764]\ttrain-rmse:0.023992\teval-rmse:0.115822\n",
      "[765]\ttrain-rmse:0.023943\teval-rmse:0.115818\n",
      "[766]\ttrain-rmse:0.0239\teval-rmse:0.115815\n",
      "[767]\ttrain-rmse:0.023866\teval-rmse:0.115818\n",
      "[768]\ttrain-rmse:0.023828\teval-rmse:0.115818\n",
      "[769]\ttrain-rmse:0.023768\teval-rmse:0.115815\n",
      "[770]\ttrain-rmse:0.023756\teval-rmse:0.115817\n",
      "[771]\ttrain-rmse:0.023726\teval-rmse:0.115816\n",
      "[772]\ttrain-rmse:0.023686\teval-rmse:0.115808\n",
      "[773]\ttrain-rmse:0.023664\teval-rmse:0.11581\n",
      "[774]\ttrain-rmse:0.023631\teval-rmse:0.115816\n",
      "[775]\ttrain-rmse:0.02361\teval-rmse:0.115807\n",
      "[776]\ttrain-rmse:0.02355\teval-rmse:0.115804\n",
      "[777]\ttrain-rmse:0.023529\teval-rmse:0.115802\n",
      "[778]\ttrain-rmse:0.023464\teval-rmse:0.115796\n",
      "[779]\ttrain-rmse:0.023428\teval-rmse:0.115794\n",
      "[780]\ttrain-rmse:0.023393\teval-rmse:0.115798\n",
      "[781]\ttrain-rmse:0.023339\teval-rmse:0.115798\n",
      "[782]\ttrain-rmse:0.023314\teval-rmse:0.115797\n",
      "[783]\ttrain-rmse:0.02328\teval-rmse:0.115801\n",
      "[784]\ttrain-rmse:0.023246\teval-rmse:0.115805\n",
      "[785]\ttrain-rmse:0.023203\teval-rmse:0.115807\n",
      "[786]\ttrain-rmse:0.023169\teval-rmse:0.115805\n",
      "[787]\ttrain-rmse:0.023131\teval-rmse:0.115797\n",
      "[788]\ttrain-rmse:0.023075\teval-rmse:0.115791\n",
      "[789]\ttrain-rmse:0.023041\teval-rmse:0.115785\n",
      "[790]\ttrain-rmse:0.023003\teval-rmse:0.115792\n",
      "[791]\ttrain-rmse:0.022966\teval-rmse:0.115786\n",
      "[792]\ttrain-rmse:0.022902\teval-rmse:0.115779\n",
      "[793]\ttrain-rmse:0.022855\teval-rmse:0.115778\n",
      "[794]\ttrain-rmse:0.022808\teval-rmse:0.115777\n",
      "[795]\ttrain-rmse:0.022774\teval-rmse:0.115777\n",
      "[796]\ttrain-rmse:0.022762\teval-rmse:0.11578\n",
      "[797]\ttrain-rmse:0.022725\teval-rmse:0.115777\n",
      "[798]\ttrain-rmse:0.022692\teval-rmse:0.115776\n",
      "[799]\ttrain-rmse:0.022655\teval-rmse:0.115779\n",
      "[800]\ttrain-rmse:0.022602\teval-rmse:0.115778\n",
      "[801]\ttrain-rmse:0.022541\teval-rmse:0.115764\n",
      "[802]\ttrain-rmse:0.022488\teval-rmse:0.11576\n",
      "[803]\ttrain-rmse:0.022453\teval-rmse:0.115756\n",
      "[804]\ttrain-rmse:0.022427\teval-rmse:0.115747\n",
      "[805]\ttrain-rmse:0.022392\teval-rmse:0.115748\n",
      "[806]\ttrain-rmse:0.022359\teval-rmse:0.115741\n",
      "[807]\ttrain-rmse:0.022302\teval-rmse:0.115744\n",
      "[808]\ttrain-rmse:0.022283\teval-rmse:0.115743\n",
      "[809]\ttrain-rmse:0.022237\teval-rmse:0.115738\n",
      "[810]\ttrain-rmse:0.022187\teval-rmse:0.115744\n",
      "[811]\ttrain-rmse:0.022173\teval-rmse:0.115738\n",
      "[812]\ttrain-rmse:0.022134\teval-rmse:0.115735\n",
      "[813]\ttrain-rmse:0.022106\teval-rmse:0.115735\n",
      "[814]\ttrain-rmse:0.02207\teval-rmse:0.115738\n",
      "[815]\ttrain-rmse:0.022052\teval-rmse:0.115736\n",
      "[816]\ttrain-rmse:0.022017\teval-rmse:0.115738\n",
      "[817]\ttrain-rmse:0.021988\teval-rmse:0.115733\n",
      "[818]\ttrain-rmse:0.021942\teval-rmse:0.115731\n",
      "[819]\ttrain-rmse:0.021891\teval-rmse:0.115734\n",
      "[820]\ttrain-rmse:0.021879\teval-rmse:0.115734\n",
      "[821]\ttrain-rmse:0.021844\teval-rmse:0.115736\n",
      "[822]\ttrain-rmse:0.021806\teval-rmse:0.115733\n",
      "[823]\ttrain-rmse:0.021794\teval-rmse:0.115734\n",
      "[824]\ttrain-rmse:0.021723\teval-rmse:0.115741\n",
      "[825]\ttrain-rmse:0.021694\teval-rmse:0.115736\n",
      "[826]\ttrain-rmse:0.02168\teval-rmse:0.115737\n",
      "[827]\ttrain-rmse:0.021663\teval-rmse:0.115736\n",
      "[828]\ttrain-rmse:0.021631\teval-rmse:0.115727\n",
      "[829]\ttrain-rmse:0.02162\teval-rmse:0.115729\n",
      "[830]\ttrain-rmse:0.021561\teval-rmse:0.115733\n",
      "[831]\ttrain-rmse:0.021533\teval-rmse:0.11573\n",
      "[832]\ttrain-rmse:0.021458\teval-rmse:0.115726\n",
      "[833]\ttrain-rmse:0.021413\teval-rmse:0.115725\n",
      "[834]\ttrain-rmse:0.021381\teval-rmse:0.115722\n",
      "[835]\ttrain-rmse:0.021323\teval-rmse:0.115723\n",
      "[836]\ttrain-rmse:0.021288\teval-rmse:0.115719\n",
      "[837]\ttrain-rmse:0.021261\teval-rmse:0.115718\n",
      "[838]\ttrain-rmse:0.021205\teval-rmse:0.115723\n",
      "[839]\ttrain-rmse:0.021169\teval-rmse:0.115721\n",
      "[840]\ttrain-rmse:0.021128\teval-rmse:0.115723\n",
      "[841]\ttrain-rmse:0.021105\teval-rmse:0.115712\n",
      "[842]\ttrain-rmse:0.021079\teval-rmse:0.115711\n",
      "[843]\ttrain-rmse:0.02105\teval-rmse:0.115699\n",
      "[844]\ttrain-rmse:0.021018\teval-rmse:0.115696\n",
      "[845]\ttrain-rmse:0.021008\teval-rmse:0.115695\n",
      "[846]\ttrain-rmse:0.020982\teval-rmse:0.115691\n",
      "[847]\ttrain-rmse:0.02096\teval-rmse:0.11568\n",
      "[848]\ttrain-rmse:0.020909\teval-rmse:0.115673\n",
      "[849]\ttrain-rmse:0.020855\teval-rmse:0.115667\n",
      "[850]\ttrain-rmse:0.020838\teval-rmse:0.115666\n",
      "[851]\ttrain-rmse:0.02082\teval-rmse:0.115657\n",
      "[852]\ttrain-rmse:0.020781\teval-rmse:0.115651\n",
      "[853]\ttrain-rmse:0.020748\teval-rmse:0.115649\n",
      "[854]\ttrain-rmse:0.020732\teval-rmse:0.115648\n",
      "[855]\ttrain-rmse:0.020701\teval-rmse:0.115648\n",
      "[856]\ttrain-rmse:0.02067\teval-rmse:0.115648\n",
      "[857]\ttrain-rmse:0.020607\teval-rmse:0.115658\n",
      "[858]\ttrain-rmse:0.020583\teval-rmse:0.115655\n",
      "[859]\ttrain-rmse:0.020574\teval-rmse:0.115653\n",
      "[860]\ttrain-rmse:0.020557\teval-rmse:0.115651\n",
      "[861]\ttrain-rmse:0.020527\teval-rmse:0.115649\n",
      "[862]\ttrain-rmse:0.020502\teval-rmse:0.11565\n",
      "[863]\ttrain-rmse:0.020448\teval-rmse:0.115648\n",
      "[864]\ttrain-rmse:0.020429\teval-rmse:0.115641\n",
      "[865]\ttrain-rmse:0.020404\teval-rmse:0.115637\n",
      "[866]\ttrain-rmse:0.020375\teval-rmse:0.115638\n",
      "[867]\ttrain-rmse:0.02035\teval-rmse:0.115635\n",
      "[868]\ttrain-rmse:0.020288\teval-rmse:0.115624\n",
      "[869]\ttrain-rmse:0.020242\teval-rmse:0.115619\n",
      "[870]\ttrain-rmse:0.020218\teval-rmse:0.115618\n",
      "[871]\ttrain-rmse:0.020156\teval-rmse:0.115611\n",
      "[872]\ttrain-rmse:0.020098\teval-rmse:0.115615\n",
      "[873]\ttrain-rmse:0.02005\teval-rmse:0.115613\n",
      "[874]\ttrain-rmse:0.020025\teval-rmse:0.115607\n",
      "[875]\ttrain-rmse:0.019996\teval-rmse:0.115599\n",
      "[876]\ttrain-rmse:0.01994\teval-rmse:0.115599\n",
      "[877]\ttrain-rmse:0.019912\teval-rmse:0.115593\n",
      "[878]\ttrain-rmse:0.019849\teval-rmse:0.115593\n",
      "[879]\ttrain-rmse:0.019821\teval-rmse:0.115584\n",
      "[880]\ttrain-rmse:0.019798\teval-rmse:0.115586\n",
      "[881]\ttrain-rmse:0.019755\teval-rmse:0.115579\n",
      "[882]\ttrain-rmse:0.019725\teval-rmse:0.115578\n",
      "[883]\ttrain-rmse:0.019698\teval-rmse:0.115577\n",
      "[884]\ttrain-rmse:0.019674\teval-rmse:0.115573\n",
      "[885]\ttrain-rmse:0.019641\teval-rmse:0.115574\n",
      "[886]\ttrain-rmse:0.019597\teval-rmse:0.115579\n",
      "[887]\ttrain-rmse:0.019544\teval-rmse:0.11558\n",
      "[888]\ttrain-rmse:0.019487\teval-rmse:0.115573\n",
      "[889]\ttrain-rmse:0.019472\teval-rmse:0.115565\n",
      "[890]\ttrain-rmse:0.01941\teval-rmse:0.115562\n",
      "[891]\ttrain-rmse:0.019367\teval-rmse:0.115562\n",
      "[892]\ttrain-rmse:0.019309\teval-rmse:0.115564\n",
      "[893]\ttrain-rmse:0.019247\teval-rmse:0.115563\n",
      "[894]\ttrain-rmse:0.019217\teval-rmse:0.115564\n",
      "[895]\ttrain-rmse:0.01919\teval-rmse:0.115557\n",
      "[896]\ttrain-rmse:0.019124\teval-rmse:0.115554\n",
      "[897]\ttrain-rmse:0.019062\teval-rmse:0.115559\n",
      "[898]\ttrain-rmse:0.019004\teval-rmse:0.11556\n",
      "[899]\ttrain-rmse:0.018932\teval-rmse:0.115566\n",
      "[900]\ttrain-rmse:0.018893\teval-rmse:0.115567\n",
      "[901]\ttrain-rmse:0.01882\teval-rmse:0.115573\n",
      "[902]\ttrain-rmse:0.018798\teval-rmse:0.115565\n",
      "[903]\ttrain-rmse:0.018758\teval-rmse:0.115564\n",
      "[904]\ttrain-rmse:0.018698\teval-rmse:0.115569\n",
      "[905]\ttrain-rmse:0.018644\teval-rmse:0.115573\n",
      "[906]\ttrain-rmse:0.01858\teval-rmse:0.115575\n",
      "[907]\ttrain-rmse:0.018541\teval-rmse:0.115575\n",
      "[908]\ttrain-rmse:0.018507\teval-rmse:0.11557\n",
      "[909]\ttrain-rmse:0.018446\teval-rmse:0.115573\n",
      "[910]\ttrain-rmse:0.018424\teval-rmse:0.11557\n",
      "[911]\ttrain-rmse:0.018385\teval-rmse:0.11557\n",
      "[912]\ttrain-rmse:0.01837\teval-rmse:0.115568\n",
      "[913]\ttrain-rmse:0.018296\teval-rmse:0.115576\n",
      "[914]\ttrain-rmse:0.018264\teval-rmse:0.11557\n",
      "[915]\ttrain-rmse:0.018249\teval-rmse:0.115568\n",
      "[916]\ttrain-rmse:0.018234\teval-rmse:0.115567\n",
      "[917]\ttrain-rmse:0.018217\teval-rmse:0.115563\n",
      "[918]\ttrain-rmse:0.018156\teval-rmse:0.11557\n",
      "[919]\ttrain-rmse:0.018133\teval-rmse:0.115568\n",
      "[920]\ttrain-rmse:0.018071\teval-rmse:0.115576\n",
      "[921]\ttrain-rmse:0.018063\teval-rmse:0.115579\n",
      "[922]\ttrain-rmse:0.018025\teval-rmse:0.115583\n",
      "[923]\ttrain-rmse:0.018012\teval-rmse:0.115587\n",
      "[924]\ttrain-rmse:0.017945\teval-rmse:0.115591\n",
      "[925]\ttrain-rmse:0.017924\teval-rmse:0.115589\n",
      "[926]\ttrain-rmse:0.01791\teval-rmse:0.115589\n",
      "[927]\ttrain-rmse:0.017845\teval-rmse:0.115595\n",
      "[928]\ttrain-rmse:0.017824\teval-rmse:0.115593\n",
      "[929]\ttrain-rmse:0.017792\teval-rmse:0.11559\n",
      "[930]\ttrain-rmse:0.01778\teval-rmse:0.115594\n",
      "[931]\ttrain-rmse:0.017758\teval-rmse:0.115588\n",
      "[932]\ttrain-rmse:0.017727\teval-rmse:0.115588\n",
      "[933]\ttrain-rmse:0.017713\teval-rmse:0.115588\n",
      "[934]\ttrain-rmse:0.0177\teval-rmse:0.115579\n",
      "[935]\ttrain-rmse:0.01768\teval-rmse:0.115577\n",
      "[936]\ttrain-rmse:0.01765\teval-rmse:0.115574\n",
      "[937]\ttrain-rmse:0.017637\teval-rmse:0.115572\n",
      "[938]\ttrain-rmse:0.017606\teval-rmse:0.115569\n",
      "[939]\ttrain-rmse:0.017562\teval-rmse:0.115565\n",
      "[940]\ttrain-rmse:0.01753\teval-rmse:0.115576\n",
      "[941]\ttrain-rmse:0.017492\teval-rmse:0.115575\n",
      "[942]\ttrain-rmse:0.017465\teval-rmse:0.115588\n",
      "[943]\ttrain-rmse:0.01745\teval-rmse:0.115581\n",
      "[944]\ttrain-rmse:0.017439\teval-rmse:0.115574\n",
      "[945]\ttrain-rmse:0.017414\teval-rmse:0.115578\n",
      "[946]\ttrain-rmse:0.017402\teval-rmse:0.115581\n",
      "[947]\ttrain-rmse:0.017389\teval-rmse:0.115579\n",
      "[948]\ttrain-rmse:0.017356\teval-rmse:0.115585\n",
      "[949]\ttrain-rmse:0.017338\teval-rmse:0.115583\n",
      "[950]\ttrain-rmse:0.017331\teval-rmse:0.115583\n",
      "[951]\ttrain-rmse:0.017302\teval-rmse:0.115581\n",
      "[952]\ttrain-rmse:0.017282\teval-rmse:0.115586\n",
      "[953]\ttrain-rmse:0.017274\teval-rmse:0.115586\n",
      "[954]\ttrain-rmse:0.017242\teval-rmse:0.115592\n",
      "[955]\ttrain-rmse:0.017234\teval-rmse:0.11559\n",
      "[956]\ttrain-rmse:0.01721\teval-rmse:0.115587\n",
      "[957]\ttrain-rmse:0.017202\teval-rmse:0.115585\n",
      "[958]\ttrain-rmse:0.017178\teval-rmse:0.115585\n",
      "[959]\ttrain-rmse:0.017137\teval-rmse:0.115582\n",
      "[960]\ttrain-rmse:0.017106\teval-rmse:0.115579\n",
      "[961]\ttrain-rmse:0.01708\teval-rmse:0.115577\n",
      "[962]\ttrain-rmse:0.017049\teval-rmse:0.115591\n",
      "[963]\ttrain-rmse:0.017042\teval-rmse:0.115589\n",
      "[964]\ttrain-rmse:0.017012\teval-rmse:0.115592\n",
      "[965]\ttrain-rmse:0.016974\teval-rmse:0.115592\n",
      "[966]\ttrain-rmse:0.016945\teval-rmse:0.115595\n",
      "[967]\ttrain-rmse:0.01693\teval-rmse:0.11559\n",
      "[968]\ttrain-rmse:0.016902\teval-rmse:0.115595\n",
      "[969]\ttrain-rmse:0.016862\teval-rmse:0.115597\n",
      "[970]\ttrain-rmse:0.016834\teval-rmse:0.1156\n",
      "[971]\ttrain-rmse:0.016799\teval-rmse:0.115602\n",
      "[972]\ttrain-rmse:0.016777\teval-rmse:0.115598\n",
      "[973]\ttrain-rmse:0.016763\teval-rmse:0.115592\n",
      "[974]\ttrain-rmse:0.016754\teval-rmse:0.115593\n",
      "[975]\ttrain-rmse:0.016744\teval-rmse:0.115586\n",
      "[976]\ttrain-rmse:0.016706\teval-rmse:0.115586\n",
      "[977]\ttrain-rmse:0.016686\teval-rmse:0.11559\n",
      "[978]\ttrain-rmse:0.016643\teval-rmse:0.115591\n",
      "[979]\ttrain-rmse:0.016605\teval-rmse:0.115593\n",
      "[980]\ttrain-rmse:0.016569\teval-rmse:0.115599\n",
      "[981]\ttrain-rmse:0.016541\teval-rmse:0.115602\n",
      "[982]\ttrain-rmse:0.016497\teval-rmse:0.115604\n",
      "[983]\ttrain-rmse:0.016483\teval-rmse:0.115597\n",
      "[984]\ttrain-rmse:0.016454\teval-rmse:0.115602\n",
      "[985]\ttrain-rmse:0.016415\teval-rmse:0.115596\n",
      "[986]\ttrain-rmse:0.016387\teval-rmse:0.115589\n",
      "[987]\ttrain-rmse:0.01637\teval-rmse:0.115585\n",
      "[988]\ttrain-rmse:0.016334\teval-rmse:0.115588\n",
      "[989]\ttrain-rmse:0.016294\teval-rmse:0.115592\n",
      "[990]\ttrain-rmse:0.016281\teval-rmse:0.115595\n",
      "[991]\ttrain-rmse:0.016239\teval-rmse:0.115598\n",
      "[992]\ttrain-rmse:0.016206\teval-rmse:0.115597\n",
      "[993]\ttrain-rmse:0.016193\teval-rmse:0.1156\n",
      "[994]\ttrain-rmse:0.016154\teval-rmse:0.115601\n",
      "[995]\ttrain-rmse:0.016107\teval-rmse:0.115599\n",
      "[996]\ttrain-rmse:0.016068\teval-rmse:0.115603\n",
      "[997]\ttrain-rmse:0.016056\teval-rmse:0.115598\n",
      "[998]\ttrain-rmse:0.016047\teval-rmse:0.11559\n",
      "[999]\ttrain-rmse:0.016033\teval-rmse:0.115586\n",
      "[1000]\ttrain-rmse:0.016018\teval-rmse:0.115585\n",
      "[1001]\ttrain-rmse:0.015994\teval-rmse:0.115592\n",
      "[1002]\ttrain-rmse:0.015963\teval-rmse:0.115593\n",
      "[1003]\ttrain-rmse:0.015946\teval-rmse:0.115594\n",
      "[1004]\ttrain-rmse:0.015939\teval-rmse:0.115592\n",
      "[1005]\ttrain-rmse:0.015911\teval-rmse:0.115585\n",
      "[1006]\ttrain-rmse:0.015898\teval-rmse:0.115583\n",
      "[1007]\ttrain-rmse:0.015873\teval-rmse:0.115577\n",
      "[1008]\ttrain-rmse:0.015834\teval-rmse:0.115574\n",
      "[1009]\ttrain-rmse:0.015786\teval-rmse:0.115573\n",
      "[1010]\ttrain-rmse:0.015751\teval-rmse:0.115581\n",
      "[1011]\ttrain-rmse:0.01574\teval-rmse:0.115576\n",
      "[1012]\ttrain-rmse:0.015732\teval-rmse:0.115572\n",
      "[1013]\ttrain-rmse:0.015696\teval-rmse:0.115573\n",
      "[1014]\ttrain-rmse:0.015684\teval-rmse:0.11557\n",
      "[1015]\ttrain-rmse:0.015674\teval-rmse:0.115563\n",
      "[1016]\ttrain-rmse:0.015637\teval-rmse:0.115563\n",
      "[1017]\ttrain-rmse:0.015619\teval-rmse:0.115559\n",
      "[1018]\ttrain-rmse:0.015611\teval-rmse:0.115561\n",
      "[1019]\ttrain-rmse:0.015589\teval-rmse:0.115559\n",
      "[1020]\ttrain-rmse:0.015582\teval-rmse:0.115558\n",
      "[1021]\ttrain-rmse:0.015553\teval-rmse:0.115559\n",
      "[1022]\ttrain-rmse:0.015538\teval-rmse:0.115561\n",
      "[1023]\ttrain-rmse:0.015511\teval-rmse:0.115573\n",
      "[1024]\ttrain-rmse:0.015503\teval-rmse:0.115572\n",
      "[1025]\ttrain-rmse:0.015487\teval-rmse:0.115569\n",
      "[1026]\ttrain-rmse:0.015473\teval-rmse:0.115568\n",
      "[1027]\ttrain-rmse:0.015465\teval-rmse:0.115572\n",
      "[1028]\ttrain-rmse:0.015457\teval-rmse:0.115568\n",
      "[1029]\ttrain-rmse:0.015434\teval-rmse:0.115573\n",
      "[1030]\ttrain-rmse:0.015413\teval-rmse:0.115572\n",
      "[1031]\ttrain-rmse:0.015406\teval-rmse:0.115571\n",
      "[1032]\ttrain-rmse:0.015371\teval-rmse:0.115563\n",
      "[1033]\ttrain-rmse:0.015359\teval-rmse:0.115563\n",
      "[1034]\ttrain-rmse:0.015335\teval-rmse:0.115554\n",
      "[1035]\ttrain-rmse:0.015309\teval-rmse:0.115557\n",
      "[1036]\ttrain-rmse:0.015272\teval-rmse:0.115563\n",
      "[1037]\ttrain-rmse:0.015243\teval-rmse:0.115566\n",
      "[1038]\ttrain-rmse:0.015187\teval-rmse:0.115563\n",
      "[1039]\ttrain-rmse:0.015154\teval-rmse:0.115562\n",
      "[1040]\ttrain-rmse:0.015103\teval-rmse:0.11557\n",
      "[1041]\ttrain-rmse:0.015075\teval-rmse:0.115572\n",
      "[1042]\ttrain-rmse:0.015014\teval-rmse:0.115566\n",
      "[1043]\ttrain-rmse:0.014967\teval-rmse:0.11557\n",
      "[1044]\ttrain-rmse:0.014941\teval-rmse:0.115571\n",
      "[1045]\ttrain-rmse:0.014918\teval-rmse:0.115566\n",
      "[1046]\ttrain-rmse:0.01488\teval-rmse:0.115565\n",
      "[1047]\ttrain-rmse:0.014875\teval-rmse:0.115563\n",
      "[1048]\ttrain-rmse:0.01485\teval-rmse:0.115564\n",
      "[1049]\ttrain-rmse:0.014813\teval-rmse:0.115563\n",
      "[1050]\ttrain-rmse:0.014798\teval-rmse:0.115564\n",
      "[1051]\ttrain-rmse:0.014769\teval-rmse:0.115555\n",
      "[1052]\ttrain-rmse:0.014739\teval-rmse:0.115555\n",
      "[1053]\ttrain-rmse:0.014723\teval-rmse:0.115554\n",
      "[1054]\ttrain-rmse:0.014695\teval-rmse:0.115557\n",
      "[1055]\ttrain-rmse:0.01467\teval-rmse:0.115557\n",
      "[1056]\ttrain-rmse:0.014664\teval-rmse:0.115553\n",
      "[1057]\ttrain-rmse:0.014633\teval-rmse:0.115552\n",
      "[1058]\ttrain-rmse:0.014615\teval-rmse:0.115548\n",
      "[1059]\ttrain-rmse:0.014602\teval-rmse:0.115544\n",
      "[1060]\ttrain-rmse:0.014587\teval-rmse:0.115543\n",
      "[1061]\ttrain-rmse:0.01455\teval-rmse:0.115543\n",
      "[1062]\ttrain-rmse:0.01452\teval-rmse:0.115542\n",
      "[1063]\ttrain-rmse:0.014488\teval-rmse:0.115546\n",
      "[1064]\ttrain-rmse:0.01446\teval-rmse:0.115548\n",
      "[1065]\ttrain-rmse:0.014439\teval-rmse:0.115551\n",
      "[1066]\ttrain-rmse:0.014431\teval-rmse:0.115557\n",
      "[1067]\ttrain-rmse:0.014407\teval-rmse:0.115558\n",
      "[1068]\ttrain-rmse:0.014374\teval-rmse:0.115556\n",
      "[1069]\ttrain-rmse:0.014334\teval-rmse:0.115558\n",
      "[1070]\ttrain-rmse:0.014311\teval-rmse:0.115559\n",
      "[1071]\ttrain-rmse:0.0143\teval-rmse:0.115565\n",
      "[1072]\ttrain-rmse:0.01428\teval-rmse:0.115562\n",
      "[1073]\ttrain-rmse:0.014265\teval-rmse:0.11556\n",
      "[1074]\ttrain-rmse:0.014243\teval-rmse:0.115563\n",
      "[1075]\ttrain-rmse:0.014235\teval-rmse:0.115567\n",
      "[1076]\ttrain-rmse:0.014197\teval-rmse:0.115564\n",
      "[1077]\ttrain-rmse:0.014168\teval-rmse:0.115568\n",
      "[1078]\ttrain-rmse:0.014135\teval-rmse:0.115577\n",
      "[1079]\ttrain-rmse:0.014093\teval-rmse:0.115578\n",
      "[1080]\ttrain-rmse:0.014086\teval-rmse:0.115583\n",
      "[1081]\ttrain-rmse:0.014049\teval-rmse:0.11558\n",
      "[1082]\ttrain-rmse:0.014021\teval-rmse:0.115581\n",
      "[1083]\ttrain-rmse:0.013988\teval-rmse:0.115582\n",
      "[1084]\ttrain-rmse:0.013962\teval-rmse:0.115581\n",
      "[1085]\ttrain-rmse:0.013934\teval-rmse:0.115585\n",
      "[1086]\ttrain-rmse:0.013908\teval-rmse:0.115586\n",
      "[1087]\ttrain-rmse:0.013873\teval-rmse:0.115586\n",
      "[1088]\ttrain-rmse:0.013841\teval-rmse:0.115595\n",
      "[1089]\ttrain-rmse:0.013812\teval-rmse:0.115601\n",
      "[1090]\ttrain-rmse:0.013786\teval-rmse:0.115603\n",
      "[1091]\ttrain-rmse:0.01377\teval-rmse:0.115603\n",
      "[1092]\ttrain-rmse:0.013744\teval-rmse:0.115609\n",
      "[1093]\ttrain-rmse:0.013713\teval-rmse:0.115611\n",
      "[1094]\ttrain-rmse:0.013674\teval-rmse:0.115614\n",
      "[1095]\ttrain-rmse:0.013649\teval-rmse:0.115615\n",
      "[1096]\ttrain-rmse:0.013636\teval-rmse:0.115618\n",
      "[1097]\ttrain-rmse:0.01361\teval-rmse:0.115626\n",
      "[1098]\ttrain-rmse:0.013585\teval-rmse:0.115619\n",
      "[1099]\ttrain-rmse:0.013579\teval-rmse:0.115625\n",
      "[1100]\ttrain-rmse:0.013572\teval-rmse:0.11563\n",
      "[1101]\ttrain-rmse:0.01355\teval-rmse:0.11563\n",
      "[1102]\ttrain-rmse:0.013526\teval-rmse:0.115631\n",
      "[1103]\ttrain-rmse:0.013493\teval-rmse:0.11563\n",
      "[1104]\ttrain-rmse:0.013478\teval-rmse:0.115626\n",
      "[1105]\ttrain-rmse:0.013441\teval-rmse:0.115625\n",
      "[1106]\ttrain-rmse:0.01341\teval-rmse:0.115635\n",
      "[1107]\ttrain-rmse:0.013386\teval-rmse:0.115636\n",
      "[1108]\ttrain-rmse:0.013381\teval-rmse:0.115639\n",
      "[1109]\ttrain-rmse:0.013359\teval-rmse:0.115637\n",
      "[1110]\ttrain-rmse:0.013333\teval-rmse:0.115641\n",
      "[1111]\ttrain-rmse:0.013321\teval-rmse:0.11564\n",
      "[1112]\ttrain-rmse:0.013315\teval-rmse:0.115645\n",
      "[1113]\ttrain-rmse:0.013303\teval-rmse:0.115646\n",
      "[1114]\ttrain-rmse:0.013285\teval-rmse:0.115644\n",
      "[1115]\ttrain-rmse:0.013266\teval-rmse:0.11564\n",
      "[1116]\ttrain-rmse:0.013255\teval-rmse:0.11564\n",
      "[1117]\ttrain-rmse:0.013222\teval-rmse:0.115639\n",
      "[1118]\ttrain-rmse:0.0132\teval-rmse:0.115639\n",
      "[1119]\ttrain-rmse:0.013171\teval-rmse:0.115632\n",
      "[1120]\ttrain-rmse:0.013139\teval-rmse:0.115633\n",
      "[1121]\ttrain-rmse:0.013121\teval-rmse:0.115633\n",
      "[1122]\ttrain-rmse:0.0131\teval-rmse:0.115633\n",
      "[1123]\ttrain-rmse:0.013095\teval-rmse:0.115635\n",
      "[1124]\ttrain-rmse:0.013073\teval-rmse:0.11564\n",
      "[1125]\ttrain-rmse:0.013041\teval-rmse:0.115644\n",
      "[1126]\ttrain-rmse:0.01302\teval-rmse:0.115645\n",
      "[1127]\ttrain-rmse:0.012988\teval-rmse:0.115646\n",
      "[1128]\ttrain-rmse:0.012965\teval-rmse:0.115645\n",
      "[1129]\ttrain-rmse:0.012944\teval-rmse:0.115645\n",
      "[1130]\ttrain-rmse:0.01292\teval-rmse:0.11564\n",
      "[1131]\ttrain-rmse:0.012899\teval-rmse:0.115644\n",
      "[1132]\ttrain-rmse:0.012887\teval-rmse:0.115639\n",
      "[1133]\ttrain-rmse:0.012866\teval-rmse:0.115638\n",
      "[1134]\ttrain-rmse:0.012842\teval-rmse:0.115635\n",
      "[1135]\ttrain-rmse:0.012815\teval-rmse:0.115633\n",
      "[1136]\ttrain-rmse:0.012799\teval-rmse:0.11563\n",
      "[1137]\ttrain-rmse:0.012772\teval-rmse:0.115639\n",
      "[1138]\ttrain-rmse:0.012753\teval-rmse:0.115642\n",
      "[1139]\ttrain-rmse:0.012727\teval-rmse:0.11564\n",
      "[1140]\ttrain-rmse:0.012722\teval-rmse:0.115643\n",
      "[1141]\ttrain-rmse:0.012708\teval-rmse:0.115643\n",
      "[1142]\ttrain-rmse:0.012681\teval-rmse:0.115651\n",
      "[1143]\ttrain-rmse:0.012661\teval-rmse:0.115651\n",
      "[1144]\ttrain-rmse:0.012657\teval-rmse:0.115653\n",
      "[1145]\ttrain-rmse:0.012642\teval-rmse:0.115648\n",
      "[1146]\ttrain-rmse:0.012623\teval-rmse:0.115648\n",
      "[1147]\ttrain-rmse:0.012612\teval-rmse:0.115646\n",
      "[1148]\ttrain-rmse:0.012584\teval-rmse:0.115644\n",
      "[1149]\ttrain-rmse:0.012563\teval-rmse:0.115646\n",
      "[1150]\ttrain-rmse:0.012547\teval-rmse:0.11564\n",
      "[1151]\ttrain-rmse:0.012532\teval-rmse:0.115638\n",
      "[1152]\ttrain-rmse:0.012528\teval-rmse:0.11564\n",
      "[1153]\ttrain-rmse:0.012518\teval-rmse:0.11564\n",
      "[1154]\ttrain-rmse:0.0125\teval-rmse:0.115643\n",
      "[1155]\ttrain-rmse:0.012479\teval-rmse:0.115647\n",
      "[1156]\ttrain-rmse:0.012455\teval-rmse:0.115645\n",
      "[1157]\ttrain-rmse:0.012437\teval-rmse:0.115646\n",
      "[1158]\ttrain-rmse:0.012433\teval-rmse:0.115647\n",
      "[1159]\ttrain-rmse:0.012409\teval-rmse:0.11565\n",
      "[1160]\ttrain-rmse:0.012387\teval-rmse:0.115648\n",
      "[1161]\ttrain-rmse:0.012364\teval-rmse:0.115647\n",
      "[1162]\ttrain-rmse:0.012343\teval-rmse:0.115649\n",
      "[1163]\ttrain-rmse:0.012325\teval-rmse:0.11565\n",
      "[1164]\ttrain-rmse:0.012305\teval-rmse:0.115648\n",
      "[1165]\ttrain-rmse:0.012301\teval-rmse:0.115649\n",
      "[1166]\ttrain-rmse:0.012297\teval-rmse:0.115651\n",
      "[1167]\ttrain-rmse:0.012281\teval-rmse:0.115653\n",
      "[1168]\ttrain-rmse:0.012277\teval-rmse:0.115655\n",
      "[1169]\ttrain-rmse:0.012265\teval-rmse:0.115655\n",
      "[1170]\ttrain-rmse:0.012262\teval-rmse:0.115657\n",
      "[1171]\ttrain-rmse:0.012238\teval-rmse:0.115653\n",
      "[1172]\ttrain-rmse:0.012191\teval-rmse:0.115657\n",
      "[1173]\ttrain-rmse:0.012176\teval-rmse:0.115653\n",
      "[1174]\ttrain-rmse:0.012173\teval-rmse:0.115654\n",
      "[1175]\ttrain-rmse:0.012157\teval-rmse:0.115655\n",
      "[1176]\ttrain-rmse:0.012136\teval-rmse:0.115657\n",
      "[1177]\ttrain-rmse:0.012133\teval-rmse:0.115659\n",
      "[1178]\ttrain-rmse:0.012088\teval-rmse:0.115663\n",
      "[1179]\ttrain-rmse:0.012075\teval-rmse:0.115664\n",
      "[1180]\ttrain-rmse:0.012058\teval-rmse:0.115665\n",
      "[1181]\ttrain-rmse:0.012054\teval-rmse:0.115667\n",
      "[1182]\ttrain-rmse:0.012039\teval-rmse:0.115668\n",
      "[1183]\ttrain-rmse:0.012015\teval-rmse:0.115669\n",
      "[1184]\ttrain-rmse:0.012011\teval-rmse:0.115669\n",
      "[1185]\ttrain-rmse:0.012001\teval-rmse:0.115668\n",
      "[1186]\ttrain-rmse:0.011978\teval-rmse:0.115672\n",
      "[1187]\ttrain-rmse:0.011975\teval-rmse:0.115673\n",
      "[1188]\ttrain-rmse:0.011967\teval-rmse:0.115671\n",
      "[1189]\ttrain-rmse:0.011932\teval-rmse:0.115675\n",
      "[1190]\ttrain-rmse:0.011929\teval-rmse:0.115677\n",
      "[1191]\ttrain-rmse:0.011912\teval-rmse:0.115678\n",
      "[1192]\ttrain-rmse:0.011888\teval-rmse:0.115676\n",
      "[1193]\ttrain-rmse:0.011871\teval-rmse:0.115678\n",
      "[1194]\ttrain-rmse:0.011845\teval-rmse:0.115676\n",
      "[1195]\ttrain-rmse:0.011842\teval-rmse:0.115677\n",
      "[1196]\ttrain-rmse:0.011806\teval-rmse:0.115686\n",
      "[1197]\ttrain-rmse:0.011778\teval-rmse:0.115689\n",
      "[1198]\ttrain-rmse:0.011764\teval-rmse:0.115686\n",
      "[1199]\ttrain-rmse:0.01176\teval-rmse:0.115687\n",
      "[1200]\ttrain-rmse:0.011737\teval-rmse:0.115686\n",
      "[1201]\ttrain-rmse:0.011734\teval-rmse:0.115687\n",
      "[1202]\ttrain-rmse:0.011715\teval-rmse:0.115689\n",
      "[1203]\ttrain-rmse:0.011681\teval-rmse:0.115694\n",
      "[1204]\ttrain-rmse:0.011668\teval-rmse:0.115691\n",
      "[1205]\ttrain-rmse:0.011633\teval-rmse:0.115695\n",
      "[1206]\ttrain-rmse:0.01162\teval-rmse:0.115692\n",
      "[1207]\ttrain-rmse:0.011593\teval-rmse:0.115694\n",
      "[1208]\ttrain-rmse:0.01158\teval-rmse:0.115689\n",
      "[1209]\ttrain-rmse:0.01155\teval-rmse:0.115689\n",
      "[1210]\ttrain-rmse:0.011529\teval-rmse:0.115686\n",
      "[1211]\ttrain-rmse:0.011508\teval-rmse:0.115691\n",
      "[1212]\ttrain-rmse:0.011496\teval-rmse:0.115689\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# Main code\n",
    "###########################################################################\n",
    "\n",
    "num_features = None # Choose how many features you want to use. None = all\n",
    "\n",
    "train = pd.read_csv(r\"C:\\Users\\piush\\Desktop\\Dataset\\House_Prices\\train.csv\") # read train data\n",
    "test = pd.read_csv(r\"C:\\Users\\piush\\Desktop\\Dataset\\House_Prices\\test.csv\") # read test data\n",
    "\n",
    "train,test,features = process_features(train,test)\n",
    "\n",
    "#test_prediction,score = train_and_test_linear(train,test,features)\n",
    "#test_prediction,score = train_and_test_tree(train,test,features) # run at least once this one to get the features importance\n",
    "#features=np.load(\"features_importance.npy\")\n",
    "test_prediction,score = train_and_test_Kfold(train,test,features[:num_features]) \n",
    "\n",
    "write_to_csv(np.exp(test_prediction),score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
