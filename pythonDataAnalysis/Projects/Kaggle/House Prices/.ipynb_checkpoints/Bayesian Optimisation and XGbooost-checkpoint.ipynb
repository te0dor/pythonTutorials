{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "from scipy.stats import skew\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "1460 rows for training set\n",
      "1459 rows for test set\n"
     ]
    }
   ],
   "source": [
    "#Import Data\n",
    "df_train = pd.read_csv(r\"C:\\Users\\piush\\Desktop\\Dataset\\House_Prices\\train.csv\")\n",
    "df_test = pd.read_csv(r\"C:\\Users\\piush\\Desktop\\Dataset\\House_Prices\\test.csv\")\n",
    "print ('data loaded')\n",
    "print (str(len(df_train))+\" rows for training set\")\n",
    "print (str(len(df_test))+\" rows for test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Median Absolute Deviation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_outlier(points, thresh = 3.5):\n",
    "    if len(points.shape) == 1:\n",
    "        points = points[:,None]\n",
    "    median = np.median(points, axis=0)\n",
    "    diff = np.sum((points - median)**2, axis=-1)\n",
    "    diff = np.sqrt(diff)\n",
    "    med_abs_deviation = np.median(diff)\n",
    "\n",
    "    modified_z_score = 0.6745 * diff / med_abs_deviation\n",
    "\n",
    "    return modified_z_score > thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Skew from SalesPrice data as required by the competition\n",
    "Select the last column as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = df_train[df_train.columns.values[-1]]\n",
    "target_log = np.log(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Train and Test to evaluate ranges and missing values excluding the last column\n",
    "This was done primarily to ensure that Categorical data in the training and testing data sets were consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train[df_train.columns.values[:-1]]\n",
    "df = df_train.append(df_test, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cats = []\n",
    "for col in df.columns.values:\n",
    "    if df[col].dtype == 'object':\n",
    "        cats.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create separte datasets for Continuous vs Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_cont = df.drop(cats, axis=1)\n",
    "df_cat = df[cats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Data for continuous data\n",
    "1. If any column contains more than 50 entries of missing data, drop the column\n",
    "2. If any column contains fewer that 50 entries of missing data, replace those missing values with the median for that column (the median imputation used on missing values is very crude. For example, Area features with missing values may be this way because the property does not have that feature (e.g. a pool) so it would make more sense to set this to zero. )\n",
    "3. Remove outliers using Median Absolute Deviation\n",
    "4. Calculate skewness for each variable and if greater than 0.75 transform it\n",
    "5. Apply the sklearn.Normalizer to each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in df_cont.columns.values:\n",
    "    if np.sum(df_cont[col].isnull()) > 50:\n",
    "        #print(\"Removing Column: {}\".format(col))\n",
    "        df_cont = df_cont.drop(col, axis = 1)\n",
    "    elif np.sum(df_cont[col].isnull()) > 0:\n",
    "        #print(\"Replacing with Median: {}\".format(col))\n",
    "        median = df_cont[col].median()\n",
    "        idx = np.where(df_cont[col].isnull())[0]\n",
    "        df_cont[col].iloc[idx] = median\n",
    "        \n",
    "        \n",
    "        outliers = np.where(is_outlier(df_cont[col]))\n",
    "        df_cont[col].iloc[outliers] = median\n",
    "        \n",
    "               \n",
    "        if skew(df_cont[col]) > 0.75:\n",
    "            #print(\"Skewness Detected: {}\".format(col))\n",
    "            df_cont[col] = np.log(df_cont[col])\n",
    "            df_cont[col] = df_cont[col].apply(lambda x: 0 if x == -np.inf else x)\n",
    "        \n",
    "        df_cont[col] = Normalizer().fit_transform(df_cont[col].reshape(1,-1))[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Data for Categorical Data\n",
    "1. If any column contains more than 50 entries of missing data, drop the column\n",
    "2. If any column contains fewer that 50 entries of missing data, replace those values with the 'MIA'\n",
    "3. Apply the sklearn.LabelEncoder\n",
    "4. For each categorical variable determine the number of unique values and for each, create a new column that is binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in df_cat.columns.values:\n",
    "    if np.sum(df_cat[col].isnull()) > 50:\n",
    "        df_cat = df_cat.drop(col, axis = 1)\n",
    "        continue\n",
    "    elif np.sum(df_cat[col].isnull()) > 0:\n",
    "        df_cat[col] = df_cat[col].fillna('MIA')\n",
    "        \n",
    "    df_cat[col] = LabelEncoder().fit_transform(df_cat[col])\n",
    "    \n",
    "    num_cols = df_cat[col].max()\n",
    "    for i in range(num_cols):\n",
    "        col_name = col + '_' + str(i)\n",
    "        df_cat[col_name] = df_cat[col].apply(lambda x: 1 if x == i else 0)\n",
    "        \n",
    "    df_cat = df_cat.drop(col, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Numeric and Categorical Datasets and Create Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new = df_cont.join(df_cat)\n",
    "\n",
    "df_train = df_new.iloc[:len(df_train) - 1]\n",
    "df_train = df_train.join(target_log)\n",
    "\n",
    "df_test = df_new.iloc[len(df_train) + 1:]\n",
    "\n",
    "X_train = df_train[df_train.columns.values[1:-1]]\n",
    "y_train = df_train[df_train.columns.values[-1]]\n",
    "\n",
    "X_test = df_test[df_test.columns.values[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the length for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1459 rows for training set\n",
      "1459 rows for test set\n"
     ]
    }
   ],
   "source": [
    "print (str(len(y_train))+\" rows for training set\")\n",
    "print (str(len(X_train))+\" rows for test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf_random = RandomForestRegressor(n_estimators=500, n_jobs=-1)\n",
    "\n",
    "clf_random.fit(X_train, y_train)\n",
    "y_pred = np.expm1(clf_random.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solution = pd.DataFrame({\"id\":df_test.Id, \"SalePrice\":y_pred}, columns=['id', 'SalePrice'])\n",
    "solution.to_csv(\"random_regressor.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
