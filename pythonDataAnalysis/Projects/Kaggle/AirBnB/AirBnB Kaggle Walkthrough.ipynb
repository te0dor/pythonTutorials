{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the data provided for the Airbnb Kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Involves looking at the data and answering a range of questions including (but not limited to):\n",
    "\n",
    "1. What features (columns) does the dataset contain?\n",
    "\n",
    "2. How many records (rows) have been provided?\n",
    "\n",
    "3. What format is the data in (e.g. what format are the dates provided, are there numerical values, what do the different categorical values look like)?\n",
    "\n",
    "4. Are there missing values?\n",
    "\n",
    "5. How do the different features relate to each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. train_users_2.csv – This dataset contains data on Airbnb users, including the destination countries. Each row represents one user with the columns containing various information such the users’ ages and when they signed up. This is the primary dataset that we will use to train the model.\n",
    "\n",
    "2. test_users.csv – This dataset also contains data on Airbnb users, in the same format as train_users_2.csv, except without the destination country. These are the users for which we will have to make our final predictions.\n",
    "\n",
    "3. sessions.csv – This data is supplementary data that can be used to train the model and make the final predictions. It contains information about the actions (e.g. clicked on a listing, updated a  wish list, ran a search etc.) taken by the users in both the testing and training datasets above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in data ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Import data\n",
    "print(\"Reading in data ...\")\n",
    "tr_filepath = 'train_users_2.csv'\n",
    "df_train = pd.read_csv(tr_filepath,header = 0,index_col = None)\n",
    "tr_filepath = 'test_users.csv'\n",
    "df_test = pd.read_csv(tr_filepath,header = 0,index_col = None)\n",
    "\n",
    "# Combine into one dataset\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fixing date formats is relatively easy. Pandas has a simple function, to_datetime, that will allow us to input a column and get the correctly formatted dates as a result. When using this function we also provide a parameter called ‘format’ that is like a regular expression for dates. In simpler terms, we are providing the function with a generalized form of the date so that it can interpret the data in the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing timestamps...\n"
     ]
    }
   ],
   "source": [
    "# Change Dates to consistent format\n",
    "print(\"Fixing timestamps...\")\n",
    "df_all['date_account_created'] = pd.to_datetime(df_all['date_account_created'], format='%Y-%m-%d')\n",
    "df_all['timestamp_first_active'] = pd.to_datetime(df_all['timestamp_first_active'], format='%Y%m%d%H%M%S')\n",
    "df_all['date_account_created'].fillna(df_all.timestamp_first_active, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove date_first_booking column\n",
    "df_all.drop('date_first_booking', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have converted the incorrect age values to NaN, we then change all the NaN values to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Remove outliers function\n",
    "def remove_outliers(df, column, min_val, max_val):\n",
    "    col_values = df[column].values\n",
    "    df[column] = np.where(np.logical_or(col_values<=min_val, col_values>=max_val), np.NaN, col_values)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing age column...\n"
     ]
    }
   ],
   "source": [
    "# Fixing age column\n",
    "print(\"Fixing age column...\")\n",
    "df_all = remove_outliers(df=df_all, column='age', min_val=15, max_val=90)\n",
    "df_all['age'].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are several more complicated ways to fill in the missing values in the age column. We are selecting this simple method for two main reasons:\n",
    "\n",
    "1. Clarity – this series of articles is going to be long enough without adding the complication of a complex methodology for imputing missing ages.\n",
    "\n",
    "2. Questionable results – in my testing during the actual competition, I did test several more complex imputation methodologies. However, none of the methods I tested actually produced a better end result than the methodology outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling first_affiliate_tracked column...\n"
     ]
    }
   ],
   "source": [
    "# Fill first_affiliate_tracked column\n",
    "print(\"Filling first_affiliate_tracked column...\")\n",
    "df_all['first_affiliate_tracked'].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>affiliate_channel</th>\n",
       "      <th>affiliate_provider</th>\n",
       "      <th>age</th>\n",
       "      <th>country_destination</th>\n",
       "      <th>date_account_created</th>\n",
       "      <th>first_affiliate_tracked</th>\n",
       "      <th>first_browser</th>\n",
       "      <th>first_device_type</th>\n",
       "      <th>gender</th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>signup_app</th>\n",
       "      <th>signup_flow</th>\n",
       "      <th>signup_method</th>\n",
       "      <th>timestamp_first_active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NDF</td>\n",
       "      <td>2010-06-28</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>gxn3p5htnn</td>\n",
       "      <td>en</td>\n",
       "      <td>Web</td>\n",
       "      <td>0</td>\n",
       "      <td>facebook</td>\n",
       "      <td>2009-03-19 04:32:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seo</td>\n",
       "      <td>google</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NDF</td>\n",
       "      <td>2011-05-25</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>MALE</td>\n",
       "      <td>820tgsjxq7</td>\n",
       "      <td>en</td>\n",
       "      <td>Web</td>\n",
       "      <td>0</td>\n",
       "      <td>facebook</td>\n",
       "      <td>2009-05-23 17:48:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>56.0</td>\n",
       "      <td>US</td>\n",
       "      <td>2010-09-28</td>\n",
       "      <td>untracked</td>\n",
       "      <td>IE</td>\n",
       "      <td>Windows Desktop</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>4ft3gnwmtx</td>\n",
       "      <td>en</td>\n",
       "      <td>Web</td>\n",
       "      <td>3</td>\n",
       "      <td>basic</td>\n",
       "      <td>2009-06-09 23:12:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>42.0</td>\n",
       "      <td>other</td>\n",
       "      <td>2011-12-05</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>bjjt8pjhuk</td>\n",
       "      <td>en</td>\n",
       "      <td>Web</td>\n",
       "      <td>0</td>\n",
       "      <td>facebook</td>\n",
       "      <td>2009-10-31 06:01:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>41.0</td>\n",
       "      <td>US</td>\n",
       "      <td>2010-09-14</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>87mebub9p4</td>\n",
       "      <td>en</td>\n",
       "      <td>Web</td>\n",
       "      <td>0</td>\n",
       "      <td>basic</td>\n",
       "      <td>2009-12-08 06:11:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>US</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>omg</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>osr2jwljor</td>\n",
       "      <td>en</td>\n",
       "      <td>Web</td>\n",
       "      <td>0</td>\n",
       "      <td>basic</td>\n",
       "      <td>2010-01-01 21:56:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>other</td>\n",
       "      <td>craigslist</td>\n",
       "      <td>46.0</td>\n",
       "      <td>US</td>\n",
       "      <td>2010-01-02</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>lsw9q7uk0j</td>\n",
       "      <td>en</td>\n",
       "      <td>Web</td>\n",
       "      <td>0</td>\n",
       "      <td>basic</td>\n",
       "      <td>2010-01-02 01:25:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>47.0</td>\n",
       "      <td>US</td>\n",
       "      <td>2010-01-03</td>\n",
       "      <td>omg</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>0d01nltbrs</td>\n",
       "      <td>en</td>\n",
       "      <td>Web</td>\n",
       "      <td>0</td>\n",
       "      <td>basic</td>\n",
       "      <td>2010-01-03 19:19:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>other</td>\n",
       "      <td>craigslist</td>\n",
       "      <td>50.0</td>\n",
       "      <td>US</td>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>a1vcnhxeij</td>\n",
       "      <td>en</td>\n",
       "      <td>Web</td>\n",
       "      <td>0</td>\n",
       "      <td>basic</td>\n",
       "      <td>2010-01-04 00:42:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>other</td>\n",
       "      <td>craigslist</td>\n",
       "      <td>46.0</td>\n",
       "      <td>US</td>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>omg</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>6uh8zyj2gn</td>\n",
       "      <td>en</td>\n",
       "      <td>Web</td>\n",
       "      <td>0</td>\n",
       "      <td>basic</td>\n",
       "      <td>2010-01-04 02:37:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>other</td>\n",
       "      <td>craigslist</td>\n",
       "      <td>36.0</td>\n",
       "      <td>US</td>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>yuuqmid2rp</td>\n",
       "      <td>en</td>\n",
       "      <td>Web</td>\n",
       "      <td>0</td>\n",
       "      <td>basic</td>\n",
       "      <td>2010-01-04 19:42:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>other</td>\n",
       "      <td>craigslist</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NDF</td>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>untracked</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>om1ss59ys8</td>\n",
       "      <td>en</td>\n",
       "      <td>Web</td>\n",
       "      <td>0</td>\n",
       "      <td>basic</td>\n",
       "      <td>2010-01-05 05:18:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>FR</td>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>-1</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>Other/Unknown</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>k6np330cm1</td>\n",
       "      <td>en</td>\n",
       "      <td>Web</td>\n",
       "      <td>0</td>\n",
       "      <td>basic</td>\n",
       "      <td>2010-01-05 06:08:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>other</td>\n",
       "      <td>craigslist</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NDF</td>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>linked</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>dy3rgx56cu</td>\n",
       "      <td>en</td>\n",
       "      <td>Web</td>\n",
       "      <td>0</td>\n",
       "      <td>basic</td>\n",
       "      <td>2010-01-05 08:32:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>other</td>\n",
       "      <td>craigslist</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NDF</td>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Mobile Safari</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>ju3h98ch3w</td>\n",
       "      <td>en</td>\n",
       "      <td>Web</td>\n",
       "      <td>0</td>\n",
       "      <td>basic</td>\n",
       "      <td>2010-01-07 05:58:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   affiliate_channel affiliate_provider   age country_destination  \\\n",
       "0             direct             direct  -1.0                 NDF   \n",
       "1                seo             google  38.0                 NDF   \n",
       "2             direct             direct  56.0                  US   \n",
       "3             direct             direct  42.0               other   \n",
       "4             direct             direct  41.0                  US   \n",
       "5              other              other  -1.0                  US   \n",
       "6              other         craigslist  46.0                  US   \n",
       "7             direct             direct  47.0                  US   \n",
       "8              other         craigslist  50.0                  US   \n",
       "9              other         craigslist  46.0                  US   \n",
       "10             other         craigslist  36.0                  US   \n",
       "11             other         craigslist  47.0                 NDF   \n",
       "12            direct             direct  -1.0                  FR   \n",
       "13             other         craigslist  37.0                 NDF   \n",
       "14             other         craigslist  36.0                 NDF   \n",
       "\n",
       "   date_account_created first_affiliate_tracked  first_browser  \\\n",
       "0            2010-06-28               untracked         Chrome   \n",
       "1            2011-05-25               untracked         Chrome   \n",
       "2            2010-09-28               untracked             IE   \n",
       "3            2011-12-05               untracked        Firefox   \n",
       "4            2010-09-14               untracked         Chrome   \n",
       "5            2010-01-01                     omg         Chrome   \n",
       "6            2010-01-02               untracked         Safari   \n",
       "7            2010-01-03                     omg         Safari   \n",
       "8            2010-01-04               untracked         Safari   \n",
       "9            2010-01-04                     omg        Firefox   \n",
       "10           2010-01-04               untracked        Firefox   \n",
       "11           2010-01-05               untracked      -unknown-   \n",
       "12           2010-01-05                      -1      -unknown-   \n",
       "13           2010-01-05                  linked        Firefox   \n",
       "14           2010-01-07               untracked  Mobile Safari   \n",
       "\n",
       "   first_device_type     gender          id language signup_app  signup_flow  \\\n",
       "0        Mac Desktop  -unknown-  gxn3p5htnn       en        Web            0   \n",
       "1        Mac Desktop       MALE  820tgsjxq7       en        Web            0   \n",
       "2    Windows Desktop     FEMALE  4ft3gnwmtx       en        Web            3   \n",
       "3        Mac Desktop     FEMALE  bjjt8pjhuk       en        Web            0   \n",
       "4        Mac Desktop  -unknown-  87mebub9p4       en        Web            0   \n",
       "5        Mac Desktop  -unknown-  osr2jwljor       en        Web            0   \n",
       "6        Mac Desktop     FEMALE  lsw9q7uk0j       en        Web            0   \n",
       "7        Mac Desktop     FEMALE  0d01nltbrs       en        Web            0   \n",
       "8        Mac Desktop     FEMALE  a1vcnhxeij       en        Web            0   \n",
       "9        Mac Desktop  -unknown-  6uh8zyj2gn       en        Web            0   \n",
       "10       Mac Desktop     FEMALE  yuuqmid2rp       en        Web            0   \n",
       "11            iPhone     FEMALE  om1ss59ys8       en        Web            0   \n",
       "12     Other/Unknown  -unknown-  k6np330cm1       en        Web            0   \n",
       "13       Mac Desktop     FEMALE  dy3rgx56cu       en        Web            0   \n",
       "14            iPhone     FEMALE  ju3h98ch3w       en        Web            0   \n",
       "\n",
       "   signup_method timestamp_first_active  \n",
       "0       facebook    2009-03-19 04:32:55  \n",
       "1       facebook    2009-05-23 17:48:09  \n",
       "2          basic    2009-06-09 23:12:47  \n",
       "3       facebook    2009-10-31 06:01:29  \n",
       "4          basic    2009-12-08 06:11:05  \n",
       "5          basic    2010-01-01 21:56:19  \n",
       "6          basic    2010-01-02 01:25:58  \n",
       "7          basic    2010-01-03 19:19:05  \n",
       "8          basic    2010-01-04 00:42:11  \n",
       "9          basic    2010-01-04 02:37:58  \n",
       "10         basic    2010-01-04 19:42:51  \n",
       "11         basic    2010-01-05 05:18:12  \n",
       "12         basic    2010-01-05 06:08:59  \n",
       "13         basic    2010-01-05 08:32:59  \n",
       "14         basic    2010-01-07 05:58:20  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main purpose of data transformation and feature extraction is to enhance the data in such a way that it increases the likelihood that the classification algorithm will be able to make meaningful predictions. Unlike the steps taken during cleaning, which are designed to address problems with the raw data (missing and erroneous values, formatting issues etc.), these steps change the values and/or structure of the data (data transformation) and add additional features (feature extraction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no textbook or walkthrough that can tell you exactly what steps you should take for a given dataset, that knowledge can come only from experience, curiosity and trial and error. However, we can take a look at some common methods to provide a sense of what is possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data transformation is undertaken with the intention to enhance the ability of the classification algorithm to extract information from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketing/Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common method for manipulating numeric data, binning or bucketing is when the numerical values in a particular column are converted from a continuous series into fixed ranges. For example, instead of using the age value of all our users, we could place them into buckets such as 15-20 years old, 21-25 years old and so on.\n",
    "\n",
    "Typically this technique is used to manage ‘noisy data’. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are concerned that small changes in a given value may simply be representing random ‘noise’, you may want to consider bucketing/binning to remove that noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Normalization_(statistics)\n",
    "\n",
    "Statistical normalization of a data is a type of feature scaling used before applying a learning algorithm. Feature scaling ( also called data normalization) is a data preprocessing method used to standardize the range of independent variables or features of a data. \n",
    "\n",
    "https://www.quora.com/In-data-mining-and-statistical-data-analysis-when-do-I-need-to-normalize-data-statistical-normalization-and-why-is-it-important-to-do-so\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Mathematical Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is an almost unlimited number of ways that the numerical values of a given column can be transformed such that they are more suitable for the algorithm being used.\n",
    "\n",
    "To provide one example, arguably the most common transformation (other than normalization) is to use a logarithm function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Data_transformation_(statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this transformation is one used for categorical data. What this transformation does is take one column with x categories (x must be greater than 2 for this to make sense) and convert it into x columns where each column represents one category in the original column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those familiar with regression modeling, you may recognize this as the same process of creating dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, the Scikit Learn library comes with a One Hot Encoder method that we could use to do these transformations.\n",
    "\n",
    "The code snippet below creates a simple function to do the encoding for a specified column, and then uses that function in a loop to convert all the categorical columns (and then delete the original columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encoding categorical data...\n"
     ]
    }
   ],
   "source": [
    "# Home made One Hot Encoding function\n",
    "def convert_to_binary(df, column_to_convert):\n",
    "    categories = list(df[column_to_convert].drop_duplicates())\n",
    "\n",
    "    for category in categories:\n",
    "        cat_name = str(category).replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\").replace(\"-\", \"\").lower()\n",
    "        col_name = column_to_convert[:5] + '_' + cat_name[:10]\n",
    "        df[col_name] = 0\n",
    "        df.loc[(df[column_to_convert] == category), col_name] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "# One Hot Encoding\n",
    "print(\"One Hot Encoding categorical data...\")\n",
    "columns_to_convert = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser']\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    df_all = convert_to_binary(df=df_all, column_to_convert=column)\n",
    "    df_all.drop(column, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often broken down into sub steps of feature construction and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Hierarchical Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will sometimes be the case that data in your dataset represents one level of a particular hierarchy, and that extracting the other implied levels of that hierarchy will provide the model with useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common ways to do this is with date fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The point is not to be able to explain why a factor may be important, but to think of as many factors as possible to test, and allow the algorithm to determine what is important and not important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding External Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using techniques such as record linkage, existing datasets can be greatly expanded by adding new data points for a given record. This new data often provides valuable new information that the algorithm can use to make more accurate predictions.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Record_linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key point here is that it is worth investing time looking for ways to add new and useful data to your existing dataset before moving onto the modeling step. Expending your dataset in this manner will often produce far bigger improvements in prediction accuracy than the choice of algorithm or the tuning of the algorithm parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for extracting a range of different data points from these two date columns date_account_created and timestamp_first_active is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding new fields...\n"
     ]
    }
   ],
   "source": [
    "# Add new date related fields\n",
    "print(\"Adding new fields...\")\n",
    "df_all['day_account_created'] = df_all['date_account_created'].dt.weekday\n",
    "df_all['month_account_created'] = df_all['date_account_created'].dt.month\n",
    "df_all['quarter_account_created'] = df_all['date_account_created'].dt.quarter\n",
    "df_all['year_account_created'] = df_all['date_account_created'].dt.year\n",
    "df_all['hour_first_active'] = df_all['timestamp_first_active'].dt.hour\n",
    "df_all['day_first_active'] = df_all['timestamp_first_active'].dt.weekday\n",
    "df_all['month_first_active'] = df_all['timestamp_first_active'].dt.month\n",
    "df_all['quarter_first_active'] = df_all['timestamp_first_active'].dt.quarter\n",
    "df_all['year_first_active'] = df_all['timestamp_first_active'].dt.year\n",
    "df_all['created_less_active'] = (df_all['date_account_created'] - df_all['timestamp_first_active']).dt.days\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['date_account_created', 'timestamp_first_active', 'date_first_booking']\n",
    "for column in columns_to_drop:\n",
    "    if column in df_all.columns:\n",
    "        df_all.drop(column, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Importance of Domain Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how the data was collected helps to provide insight into potential errors in the data that might need to be addressed or shortcomings in the way the data was sampled (sample selection bias/errors). Understanding the relevant industry or market can also provide a range of insights including:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. what additional information is available to expand your dataset\n",
    "\n",
    "2. what information may help to increase prediction accuracy and what is likely to be irrelevant\n",
    "\n",
    "3. if the model makes intuitive sense (e.g. can you predict the likelihood of a waking up with a headache based on whether someone slept with their shoes on?[1]), and\n",
    "\n",
    "4. if the industry or market is changing in such a way that it is likely to make the model redundant in the near future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import sessions data\n",
    "s_filepath = \"sessions.csv\"\n",
    "sessions = pd.read_csv(s_filepath, header=0, index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the primary and secondary devices for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determing primary device...\n",
      "Determing secondary device...\n"
     ]
    }
   ],
   "source": [
    "# Determine primary device\n",
    "print(\"Determing primary device...\")\n",
    "sessions_device = sessions.loc[:, ['user_id', 'device_type', 'secs_elapsed']]\n",
    "aggregated_lvl1 = sessions_device.groupby(['user_id', 'device_type'], as_index=False, sort=False).aggregate(np.sum)\n",
    "idx = aggregated_lvl1.groupby(['user_id'], sort=False)['secs_elapsed'].transform(max) == aggregated_lvl1['secs_elapsed']\n",
    "df_primary = pd.DataFrame(aggregated_lvl1.loc[idx , ['user_id', 'device_type', 'secs_elapsed']])\n",
    "df_primary.rename(columns = {'device_type':'primary_device', 'secs_elapsed':'primary_secs'}, inplace=True)\n",
    "df_primary = convert_to_binary(df=df_primary, column_to_convert='primary_device')\n",
    "df_primary.drop('primary_device', axis=1, inplace=True)\n",
    "\n",
    "# Determine Secondary device\n",
    "print(\"Determing secondary device...\")\n",
    "remaining = aggregated_lvl1.drop(aggregated_lvl1.index[idx])\n",
    "idx = remaining.groupby(['user_id'], sort=False)['secs_elapsed'].transform(max) == remaining['secs_elapsed']\n",
    "df_secondary = pd.DataFrame(remaining.loc[idx , ['user_id', 'device_type', 'secs_elapsed']])\n",
    "df_secondary.rename(columns = {'device_type':'secondary_device', 'secs_elapsed':'secondary_secs'}, inplace=True)\n",
    "df_secondary = convert_to_binary(df=df_secondary, column_to_convert='secondary_device')\n",
    "df_secondary.drop('secondary_device', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Counts of Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we are going to do is take counts of how many times each action was taken by each user. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "being able to identify techniques you have used in other programs and languages provides you with a way to find corresponding methods in new languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating actions taken...\n",
      "Converting action column...\n",
      "Converting action_type column...\n",
      "Converting action_detail column...\n"
     ]
    }
   ],
   "source": [
    "# Count occurrences of value in a column\n",
    "def convert_to_counts(df, id_col, column_to_convert):\n",
    "    id_list = df[id_col].drop_duplicates()\n",
    "    \n",
    "    df_counts = df.loc[:,[id_col, column_to_convert]]\n",
    "    df_counts['count'] = 1\n",
    "    df_counts = df_counts.groupby(by=[id_col, column_to_convert], as_index=False, sort=False).sum()\n",
    "    \n",
    "    new_df = df_counts.pivot(index=id_col, columns=column_to_convert, values='count')\n",
    "    new_df = new_df.fillna(0)\n",
    "    \n",
    "    # Rename Columns\n",
    "    categories = list(df[column_to_convert].drop_duplicates())\n",
    "    for category in categories:\n",
    "        cat_name = str(category).replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\").replace(\"-\", \"\").lower()\n",
    "        col_name = column_to_convert + '_' + cat_name\n",
    "        new_df.rename(columns = {category:col_name}, inplace=True)\n",
    "        \n",
    "    return new_df\n",
    "\n",
    "# Aggregate and combine actions taken columns\n",
    "print(\"Aggregating actions taken...\")\n",
    "session_actions = sessions.loc[:,['user_id', 'action', 'action_type', 'action_detail']]\n",
    "columns_to_convert = ['action', 'action_type', 'action_detail']\n",
    "session_actions = session_actions.fillna('not provided')\n",
    "first = True\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    print(\"Converting \" + column + \" column...\")\n",
    "    current_data = convert_to_counts(df=session_actions, id_col='user_id', column_to_convert=column)\n",
    "\n",
    "    # If first loop, current data becomes existing data, otherwise merge existing and current\n",
    "    if first:\n",
    "        first = False\n",
    "        actions_data = current_data\n",
    "    else:\n",
    "        actions_data = pd.concat([actions_data, current_data], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine the various datasets we have created into one large dataset. First we combine the two device dataframes (df_primary and df_secondary) to create a device dataframe. Then we combine the device dataframe with the actions dataframe to create a sessions dataframe with all the features we extracted from sessions.csv. Finally, we combine the sessions dataframe with the user data dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining results...\n"
     ]
    }
   ],
   "source": [
    "# Merge device datasets\n",
    "print(\"Combining results...\")\n",
    "df_primary.set_index('user_id', inplace=True)\n",
    "df_secondary.set_index('user_id', inplace=True)\n",
    "device_data = pd.concat([df_primary, df_secondary], axis=1, join=\"outer\")\n",
    "\n",
    "# Merge device and actions datasets\n",
    "combined_results = pd.concat([device_data, actions_data], axis=1, join='outer')\n",
    "df_sessions = combined_results.fillna(0)\n",
    "\n",
    "# Merge user and session datasets\n",
    "df_all.set_index('id', inplace=True)\n",
    "df_all = pd.concat([df_all, df_sessions], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step requires an outer join because not all users have a secondary device. That is, some users only logged onto Airbnb using one device (or at least one type of device). Doing an outer join here ensures that our dataset includes all users regardless of this fact.\n",
    "\n",
    "The second step could use an inner or an outer join, as both the device and actions datasets should contain all users. In this case we use an outer join just to ensure that if a user is missing from one of the datasets (for whatever reason), we will still capture them. You may also notice that after the second step we fill any missing values with 0s to ensure we do not have any NULL values that may have been generated by these outer joins.\n",
    "\n",
    "For the third step we use an inner join for a key reason – we want our final training dataset to only include users that also have sessions data. Using an inner join here is an easy way to join the datasets and filter for the users with sessions data in one step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.codeproject.com/Articles/33052/Visual-Representation-of-SQL-Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing an Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to building a model is to decide what type of algorithm to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the decision tree algorithm used for classification problems (like the one we are looking at) is to create one of these decision trees to classify records into a set number of categories. To do this, it starts with all the records in the training dataset and looks through all the features until it finds the one that allows it to most ‘cleanly’ split the records according to their categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   1875\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1876\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1877\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4027)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:3891)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:12408)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:12359)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'id'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-466c299e32a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Prepare training data for modelling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'country_destination'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'inner'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mset_index\u001b[1;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[0;32m   2829\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2830\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2831\u001b[1;33m                 \u001b[0mlevel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2832\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2833\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1990\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1991\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1992\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2002\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2003\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2004\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1990\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1991\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1992\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1997\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1998\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1999\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2001\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1343\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1345\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1346\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3224\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3225\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3226\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3227\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   1876\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1877\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1878\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4027)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:3891)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:12408)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:12359)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Prepare training data for modelling\n",
    "df_train.set_index('id', inplace=True)\n",
    "df_train = pd.concat([df_train['country_destination'], df_all], axis=1, join='inner')\n",
    "\n",
    "id_train = df_train.index.values\n",
    "labels = df_train['country_destination']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "X = df_train.drop('country_destination', axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-90255ce21def>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#from xgboost.sklearn import XGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m   \u001b[1;31m#Additional scklearn functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_search\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m   \u001b[1;31m#Perforing grid search\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "# Grid Search - Used to find best combination of parameters\n",
    "XGB_model = xgb.XGBClassifier(objective='multi:softprob', subsample=0.5, colsample_bytree=0.5, seed=0)\n",
    "param_grid = {'max_depth': [3, 4, 5], 'learning_rate': [0.1, 0.3], 'n_estimators': [25, 50]}\n",
    "model = grid_search.GridSearchCV(estimator=XGB_model, param_grid=param_grid, scoring='accuracy', verbose=10, n_jobs=1, iid=True, refit=True, cv=3)\n",
    "\n",
    "model.fit(X, y)\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-acd5d4f50f1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Make predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare test data for prediction\n",
    "df_test.set_index('id', inplace=True)\n",
    "df_test = pd.merge(df_test.loc[:,['date_first_booking']], df_all, how='left', left_index=True, right_index=True, sort=False)\n",
    "X_test = df_test.drop('date_first_booking', axis=1, inplace=False)\n",
    "X_test = X_test.fillna(-1)\n",
    "id_test = df_test.index.values\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-2cf5125d6cd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mid_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mids\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mcts\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#Generate submission\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "#Taking the 5 classes with highest probabilities\n",
    "ids = []  #list of ids\n",
    "cts = []  #list of countries\n",
    "for i in range(len(id_test)):\n",
    "    idx = id_test[i]\n",
    "    ids += [idx] * 5\n",
    "    cts += le.inverse_transform(np.argsort(y_pred[i])[::-1])[:5].tolist()\n",
    "\n",
    "#Generate submission\n",
    "print(\"Outputting final results...\")\n",
    "sub = pd.DataFrame(np.column_stack((ids, cts)), columns=['id', 'country'])\n",
    "sub.to_csv('./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-83030391a0ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecomposition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import cross_validation, decomposition, grid_search\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "####################################################\n",
    "# Functions                                        #\n",
    "####################################################\n",
    "# Remove outliers\n",
    "def remove_outliers(df, column, min_val, max_val):\n",
    "    col_values = df[column].values\n",
    "    df[column] = np.where(np.logical_or(col_values<=min_val, col_values>=max_val), np.NaN, col_values)\n",
    "    \n",
    "    return df \n",
    "\n",
    "# Home made One Hot Encoder\n",
    "def convert_to_binary(df, column_to_convert):\n",
    "    categories = list(df[column_to_convert].drop_duplicates())\n",
    "    \n",
    "    for category in categories:\n",
    "        cat_name = str(category).replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\").replace(\"-\", \"\").lower()\n",
    "        col_name = column_to_convert[:5] + '_' + cat_name[:10]\n",
    "        df[col_name] = 0\n",
    "        df.loc[(df[column_to_convert] == category), col_name] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Count occurrences of value in a column\n",
    "def convert_to_counts(df, id_col, column_to_convert):\n",
    "    id_list = df[id_col].drop_duplicates()\n",
    "    \n",
    "    df_counts = df.loc[:,[id_col, column_to_convert]]\n",
    "    df_counts['count'] = 1\n",
    "    df_counts = df_counts.groupby(by=[id_col, column_to_convert], as_index=False, sort=False).sum()\n",
    "    \n",
    "    new_df = df_counts.pivot(index=id_col, columns=column_to_convert, values='count')\n",
    "    new_df = new_df.fillna(0)\n",
    "    \n",
    "    # Rename Columns\n",
    "    categories = list(df[column_to_convert].drop_duplicates())\n",
    "    for category in categories:\n",
    "        cat_name = str(category).replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\").replace(\"-\", \"\").lower()\n",
    "        col_name = column_to_convert + '_' + cat_name\n",
    "        new_df.rename(columns = {category:col_name}, inplace=True)\n",
    "        \n",
    "    return new_df\n",
    "\n",
    "####################################################\n",
    "# Cleaning                                         #\n",
    "####################################################\n",
    "# Import data\n",
    "print(\"Reading in data...\")\n",
    "tr_filepath = \"train_users_2.csv\"\n",
    "df_train = pd.read_csv(tr_filepath, header=0, index_col=None)\n",
    "te_filepath = \"test_users.csv\"\n",
    "df_test = pd.read_csv(te_filepath, header=0, index_col=None)\n",
    "\n",
    "# Combine into one dataset\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "\n",
    "# Change Dates to consistent format\n",
    "print(\"Fixing timestamps...\")\n",
    "df_all['date_account_created'] =  pd.to_datetime(df_all['date_account_created'], format='%Y-%m-%d')\n",
    "df_all['timestamp_first_active'] =  pd.to_datetime(df_all['timestamp_first_active'], format='%Y%m%d%H%M%S')\n",
    "df_all['date_account_created'].fillna(df_all.timestamp_first_active, inplace=True)\n",
    "\n",
    "# Remove date_first_booking column\n",
    "df_all.drop('date_first_booking', axis=1, inplace=True)\n",
    "\n",
    "# Fixing age column\n",
    "print(\"Fixing age column...\")\n",
    "df_all = remove_outliers(df=df_all, column='age', min_val=15, max_val=90)\n",
    "df_all['age'].fillna(-1, inplace=True)\n",
    "\n",
    "# Fill first_affiliate_tracked column\n",
    "print(\"Filling first_affiliate_tracked column...\")\n",
    "df_all['first_affiliate_tracked'].fillna(-1, inplace=True)\n",
    "\n",
    "####################################################\n",
    "# Data Transformation                              #\n",
    "####################################################\n",
    "# One Hot Encoding\n",
    "print(\"One Hot Encoding categorical data...\")\n",
    "columns_to_convert = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser']\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    df_all = convert_to_binary(df=df_all, column_to_convert=column)\n",
    "    df_all.drop(column, axis=1, inplace=True)\n",
    "\n",
    "####################################################\n",
    "# Feature Extraction                               #\n",
    "####################################################\n",
    "# Add new date related fields\n",
    "print(\"Adding new fields...\")\n",
    "df_all['day_account_created'] = df_all['date_account_created'].dt.weekday\n",
    "df_all['month_account_created'] = df_all['date_account_created'].dt.month\n",
    "df_all['quarter_account_created'] = df_all['date_account_created'].dt.quarter\n",
    "df_all['year_account_created'] = df_all['date_account_created'].dt.year\n",
    "df_all['hour_first_active'] = df_all['timestamp_first_active'].dt.hour\n",
    "df_all['day_first_active'] = df_all['timestamp_first_active'].dt.weekday\n",
    "df_all['month_first_active'] = df_all['timestamp_first_active'].dt.month\n",
    "df_all['quarter_first_active'] = df_all['timestamp_first_active'].dt.quarter\n",
    "df_all['year_first_active'] = df_all['timestamp_first_active'].dt.year\n",
    "df_all['created_less_active'] = (df_all['date_account_created'] - df_all['timestamp_first_active']).dt.days\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['date_account_created', 'timestamp_first_active', 'date_first_booking', 'country_destination']\n",
    "for column in columns_to_drop:\n",
    "    if column in df_all.columns:\n",
    "        df_all.drop(column, axis=1, inplace=True)\n",
    "\n",
    "####################################################\n",
    "# Add data from sessions.csv                       #\n",
    "####################################################\n",
    "# Import sessions data\n",
    "s_filepath = \"./sessions.csv\"\n",
    "sessions = pd.read_csv(s_filepath, header=0, index_col=False)\n",
    "\n",
    "# Determine primary device\n",
    "print(\"Determing primary device...\")\n",
    "sessions_device = sessions.loc[:, ['user_id', 'device_type', 'secs_elapsed']]\n",
    "aggregated_lvl1 = sessions_device.groupby(['user_id', 'device_type'], as_index=False, sort=False).aggregate(np.sum)\n",
    "idx = aggregated_lvl1.groupby(['user_id'], sort=False)['secs_elapsed'].transform(max) == aggregated_lvl1['secs_elapsed']\n",
    "df_primary = pd.DataFrame(aggregated_lvl1.loc[idx , ['user_id', 'device_type', 'secs_elapsed']])\n",
    "df_primary.rename(columns = {'device_type':'primary_device', 'secs_elapsed':'primary_secs'}, inplace=True)\n",
    "df_primary = convert_to_binary(df=df_primary, column_to_convert='primary_device')\n",
    "df_primary.drop('primary_device', axis=1, inplace=True)\n",
    "\n",
    "# Determine Secondary device\n",
    "print(\"Determing secondary device...\")\n",
    "remaining = aggregated_lvl1.drop(aggregated_lvl1.index[idx])\n",
    "idx = remaining.groupby(['user_id'], sort=False)['secs_elapsed'].transform(max) == remaining['secs_elapsed']\n",
    "df_secondary = pd.DataFrame(remaining.loc[idx , ['user_id', 'device_type', 'secs_elapsed']])\n",
    "df_secondary.rename(columns = {'device_type':'secondary_device', 'secs_elapsed':'secondary_secs'}, inplace=True)\n",
    "df_secondary = convert_to_binary(df=df_secondary, column_to_convert='secondary_device')\n",
    "df_secondary.drop('secondary_device', axis=1, inplace=True)\n",
    "\n",
    "# Aggregate and combine actions taken columns\n",
    "print(\"Aggregating actions taken...\")\n",
    "session_actions = sessions.loc[:,['user_id', 'action', 'action_type', 'action_detail']]\n",
    "columns_to_convert = ['action', 'action_type', 'action_detail']\n",
    "session_actions = session_actions.fillna('not provided')\n",
    "first = True\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    print(\"Converting \" + column + \" column...\")\n",
    "    current_data = convert_to_counts(df=session_actions, id_col='user_id', column_to_convert=column)\n",
    "\n",
    "    # If first loop, current data becomes existing data, otherwise merge existing and current\n",
    "    if first:\n",
    "        first = False\n",
    "        actions_data = current_data\n",
    "    else:\n",
    "        actions_data = pd.concat([actions_data, current_data], axis=1, join='inner')\n",
    "\n",
    "# Merge device datasets\n",
    "print(\"Combining results...\")\n",
    "df_primary.set_index('user_id', inplace=True)\n",
    "df_secondary.set_index('user_id', inplace=True)\n",
    "device_data = pd.concat([df_primary, df_secondary], axis=1, join=\"outer\")\n",
    "\n",
    "# Merge device and actions datasets\n",
    "combined_results = pd.concat([device_data, actions_data], axis=1, join='outer')\n",
    "df_sessions = combined_results.fillna(0)\n",
    "\n",
    "# Merge user and session datasets\n",
    "df_all.set_index('id', inplace=True)\n",
    "df_all = pd.concat([df_all, df_sessions], axis=1, join='inner')\n",
    "\n",
    "####################################################\n",
    "# Building Model                                   #\n",
    "####################################################\n",
    "# Prepare training data for modelling\n",
    "df_train.set_index('id', inplace=True)\n",
    "df_train = pd.concat([df_train['country_destination'], df_all], axis=1, join='inner')\n",
    "\n",
    "id_train = df_train.index.values\n",
    "labels = df_train['country_destination']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "X = df_train.drop('country_destination', axis=1, inplace=False)\n",
    "\n",
    "# Training model\n",
    "print(\"Training model...\")\n",
    "\n",
    "# Grid Search - Used to find best combination of parameters\n",
    "XGB_model = xgb.XGBClassifier(objective='multi:softprob', subsample=0.5, colsample_bytree=0.5, seed=0)\n",
    "param_grid = {'max_depth': [3, 4], 'learning_rate': [0.1, 0.3], 'n_estimators': [25, 50]}\n",
    "model = grid_search.GridSearchCV(estimator=XGB_model, param_grid=param_grid, scoring='accuracy', verbose=10, n_jobs=1, iid=True, refit=True, cv=3)\n",
    "\n",
    "model.fit(X, y)\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "####################################################\n",
    "# Make predictions                                 #\n",
    "####################################################\n",
    "print(\"Making predictions...\")\n",
    "\n",
    "# Prepare test data for prediction\n",
    "df_test.set_index('id', inplace=True)\n",
    "df_test = pd.merge(df_test.loc[:,['date_first_booking']], df_all, how='left', left_index=True, right_index=True, sort=False)\n",
    "X_test = df_test.drop('date_first_booking', axis=1, inplace=False)\n",
    "X_test = X_test.fillna(-1)\n",
    "id_test = df_test.index.values\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict_proba(X_test)\n",
    "\n",
    "#Taking the 5 classes with highest probabilities\n",
    "ids = []  #list of ids\n",
    "cts = []  #list of countries\n",
    "for i in range(len(id_test)):\n",
    "    idx = id_test[i]\n",
    "    ids += [idx] * 5\n",
    "    cts += le.inverse_transform(np.argsort(y_pred[i])[::-1])[:5].tolist()\n",
    "\n",
    "#Generate submission\n",
    "print(\"Outputting final results...\")\n",
    "sub = pd.DataFrame(np.column_stack((ids, cts)), columns=['id', 'country'])\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
